[
["index.html", "Project Template Analysis Introduction: BLueprint and Recipes for Stats analytic Project", " Project Template Analysis Tymen Schreuder 2017-07-28 Introduction: BLueprint and Recipes for Stats analytic Project "],
["prepare-problem.html", "Chapter 1 Prepare Problem 1.1 Load packages 1.2 Load dataset", " Chapter 1 Prepare Problem 1.1 Load packages 1.2 Load dataset # Datasets from the mlbench library # load the library library(mlbench) # list the contents of the library #library(help = &quot;mlbench&quot;) # Boston Housing dataset data(BostonHousing) dim(BostonHousing) ## [1] 506 14 head(BostonHousing) ## crim zn indus chas nox rm age dis rad tax ptratio b ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 # Wisconsin Breast Cancer dataset data(BreastCancer) dim(BreastCancer) ## [1] 699 11 levels(BreastCancer$Class) ## [1] &quot;benign&quot; &quot;malignant&quot; head(BreastCancer) ## Id Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size ## 1 1000025 5 1 1 1 2 ## 2 1002945 5 4 4 5 7 ## 3 1015425 3 1 1 1 2 ## 4 1016277 6 8 8 1 3 ## 5 1017023 4 1 1 3 2 ## 6 1017122 8 10 10 8 7 ## Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses Class ## 1 1 3 1 1 benign ## 2 10 3 2 1 benign ## 3 2 3 1 1 benign ## 4 4 3 7 1 benign ## 5 1 3 1 1 benign ## 6 10 9 7 1 malignant # Glass Identification dataset data(Glass) dim(Glass) ## [1] 214 10 levels(Glass$Type) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; head(Glass) ## RI Na Mg Al Si K Ca Ba Fe Type ## 1 1.52101 13.64 4.49 1.10 71.78 0.06 8.75 0 0.00 1 ## 2 1.51761 13.89 3.60 1.36 72.73 0.48 7.83 0 0.00 1 ## 3 1.51618 13.53 3.55 1.54 72.99 0.39 7.78 0 0.00 1 ## 4 1.51766 13.21 3.69 1.29 72.61 0.57 8.22 0 0.00 1 ## 5 1.51742 13.27 3.62 1.24 73.08 0.55 8.07 0 0.00 1 ## 6 1.51596 12.79 3.61 1.62 72.97 0.64 8.07 0 0.26 1 # Johns Hopkins University Ionosphere dataset data(Ionosphere) dim(Ionosphere) ## [1] 351 35 levels(Ionosphere$Class) ## [1] &quot;bad&quot; &quot;good&quot; head(Ionosphere) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 ## 1 1 0 0.99539 -0.05889 0.85243 0.02306 0.83398 -0.37708 1.00000 ## 2 1 0 1.00000 -0.18829 0.93035 -0.36156 -0.10868 -0.93597 1.00000 ## 3 1 0 1.00000 -0.03365 1.00000 0.00485 1.00000 -0.12062 0.88965 ## 4 1 0 1.00000 -0.45161 1.00000 1.00000 0.71216 -1.00000 0.00000 ## 5 1 0 1.00000 -0.02401 0.94140 0.06531 0.92106 -0.23255 0.77152 ## 6 1 0 0.02337 -0.00592 -0.09924 -0.11949 -0.00763 -0.11824 0.14706 ## V10 V11 V12 V13 V14 V15 V16 V17 ## 1 0.03760 0.85243 -0.17755 0.59755 -0.44945 0.60536 -0.38223 0.84356 ## 2 -0.04549 0.50874 -0.67743 0.34432 -0.69707 -0.51685 -0.97515 0.05499 ## 3 0.01198 0.73082 0.05346 0.85443 0.00827 0.54591 0.00299 0.83775 ## 4 0.00000 0.00000 0.00000 0.00000 0.00000 -1.00000 0.14516 0.54094 ## 5 -0.16399 0.52798 -0.20275 0.56409 -0.00712 0.34395 -0.27457 0.52940 ## 6 0.06637 0.03786 -0.06302 0.00000 0.00000 -0.04572 -0.15540 -0.00343 ## V18 V19 V20 V21 V22 V23 V24 V25 ## 1 -0.38542 0.58212 -0.32192 0.56971 -0.29674 0.36946 -0.47357 0.56811 ## 2 -0.62237 0.33109 -1.00000 -0.13151 -0.45300 -0.18056 -0.35734 -0.20332 ## 3 -0.13644 0.75535 -0.08540 0.70887 -0.27502 0.43385 -0.12062 0.57528 ## 4 -0.39330 -1.00000 -0.54467 -0.69975 1.00000 0.00000 0.00000 1.00000 ## 5 -0.21780 0.45107 -0.17813 0.05982 -0.35575 0.02309 -0.52879 0.03286 ## 6 -0.10196 -0.11575 -0.05414 0.01838 0.03669 0.01519 0.00888 0.03513 ## V26 V27 V28 V29 V30 V31 V32 V33 ## 1 -0.51171 0.41078 -0.46168 0.21266 -0.34090 0.42267 -0.54487 0.18641 ## 2 -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626 -0.06288 -0.13738 ## 3 -0.40220 0.58984 -0.22145 0.43100 -0.17365 0.60436 -0.24180 0.56045 ## 4 0.90695 0.51613 1.00000 1.00000 -0.20099 0.25682 1.00000 -0.32382 ## 5 -0.65158 0.13290 -0.53206 0.02431 -0.62197 -0.05707 -0.59573 -0.04608 ## 6 -0.01535 -0.03240 0.09223 -0.07859 0.00732 0.00000 0.00000 -0.00039 ## V34 Class ## 1 -0.45300 good ## 2 -0.02447 bad ## 3 -0.38238 good ## 4 1.00000 bad ## 5 -0.65697 good ## 6 0.12011 bad # Pima Indians Diabetes dataset data(PimaIndiansDiabetes) dim(PimaIndiansDiabetes) ## [1] 768 9 levels(PimaIndiansDiabetes$diabetes) ## [1] &quot;neg&quot; &quot;pos&quot; head(PimaIndiansDiabetes) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 0 33.6 0.627 50 pos ## 2 1 85 66 29 0 26.6 0.351 31 neg ## 3 8 183 64 0 0 23.3 0.672 32 pos ## 4 1 89 66 23 94 28.1 0.167 21 neg ## 5 0 137 40 35 168 43.1 2.288 33 pos ## 6 5 116 74 0 0 25.6 0.201 30 neg # Sonar, Mines vs. Rocks dataset data(Sonar) dim(Sonar) ## [1] 208 61 levels(Sonar$Class) ## [1] &quot;M&quot; &quot;R&quot; head(Sonar) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ## 1 0.0200 0.0371 0.0428 0.0207 0.0954 0.0986 0.1539 0.1601 0.3109 0.2111 ## 2 0.0453 0.0523 0.0843 0.0689 0.1183 0.2583 0.2156 0.3481 0.3337 0.2872 ## 3 0.0262 0.0582 0.1099 0.1083 0.0974 0.2280 0.2431 0.3771 0.5598 0.6194 ## 4 0.0100 0.0171 0.0623 0.0205 0.0205 0.0368 0.1098 0.1276 0.0598 0.1264 ## 5 0.0762 0.0666 0.0481 0.0394 0.0590 0.0649 0.1209 0.2467 0.3564 0.4459 ## 6 0.0286 0.0453 0.0277 0.0174 0.0384 0.0990 0.1201 0.1833 0.2105 0.3039 ## V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 ## 1 0.1609 0.1582 0.2238 0.0645 0.0660 0.2273 0.3100 0.2999 0.5078 0.4797 ## 2 0.4918 0.6552 0.6919 0.7797 0.7464 0.9444 1.0000 0.8874 0.8024 0.7818 ## 3 0.6333 0.7060 0.5544 0.5320 0.6479 0.6931 0.6759 0.7551 0.8929 0.8619 ## 4 0.0881 0.1992 0.0184 0.2261 0.1729 0.2131 0.0693 0.2281 0.4060 0.3973 ## 5 0.4152 0.3952 0.4256 0.4135 0.4528 0.5326 0.7306 0.6193 0.2032 0.4636 ## 6 0.2988 0.4250 0.6343 0.8198 1.0000 0.9988 0.9508 0.9025 0.7234 0.5122 ## V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 ## 1 0.5783 0.5071 0.4328 0.5550 0.6711 0.6415 0.7104 0.8080 0.6791 0.3857 ## 2 0.5212 0.4052 0.3957 0.3914 0.3250 0.3200 0.3271 0.2767 0.4423 0.2028 ## 3 0.7974 0.6737 0.4293 0.3648 0.5331 0.2413 0.5070 0.8533 0.6036 0.8514 ## 4 0.2741 0.3690 0.5556 0.4846 0.3140 0.5334 0.5256 0.2520 0.2090 0.3559 ## 5 0.4148 0.4292 0.5730 0.5399 0.3161 0.2285 0.6995 1.0000 0.7262 0.4724 ## 6 0.2074 0.3985 0.5890 0.2872 0.2043 0.5782 0.5389 0.3750 0.3411 0.5067 ## V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 ## 1 0.1307 0.2604 0.5121 0.7547 0.8537 0.8507 0.6692 0.6097 0.4943 0.2744 ## 2 0.3788 0.2947 0.1984 0.2341 0.1306 0.4182 0.3835 0.1057 0.1840 0.1970 ## 3 0.8512 0.5045 0.1862 0.2709 0.4232 0.3043 0.6116 0.6756 0.5375 0.4719 ## 4 0.6260 0.7340 0.6120 0.3497 0.3953 0.3012 0.5408 0.8814 0.9857 0.9167 ## 5 0.5103 0.5459 0.2881 0.0981 0.1951 0.4181 0.4604 0.3217 0.2828 0.2430 ## 6 0.5580 0.4778 0.3299 0.2198 0.1407 0.2856 0.3807 0.4158 0.4054 0.3296 ## V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 ## 1 0.0510 0.2834 0.2825 0.4256 0.2641 0.1386 0.1051 0.1343 0.0383 0.0324 ## 2 0.1674 0.0583 0.1401 0.1628 0.0621 0.0203 0.0530 0.0742 0.0409 0.0061 ## 3 0.4647 0.2587 0.2129 0.2222 0.2111 0.0176 0.1348 0.0744 0.0130 0.0106 ## 4 0.6121 0.5006 0.3210 0.3202 0.4295 0.3654 0.2655 0.1576 0.0681 0.0294 ## 5 0.1979 0.2444 0.1847 0.0841 0.0692 0.0528 0.0357 0.0085 0.0230 0.0046 ## 6 0.2707 0.2650 0.0723 0.1238 0.1192 0.1089 0.0623 0.0494 0.0264 0.0081 ## V51 V52 V53 V54 V55 V56 V57 V58 V59 V60 ## 1 0.0232 0.0027 0.0065 0.0159 0.0072 0.0167 0.0180 0.0084 0.0090 0.0032 ## 2 0.0125 0.0084 0.0089 0.0048 0.0094 0.0191 0.0140 0.0049 0.0052 0.0044 ## 3 0.0033 0.0232 0.0166 0.0095 0.0180 0.0244 0.0316 0.0164 0.0095 0.0078 ## 4 0.0241 0.0121 0.0036 0.0150 0.0085 0.0073 0.0050 0.0044 0.0040 0.0117 ## 5 0.0156 0.0031 0.0054 0.0105 0.0110 0.0015 0.0072 0.0048 0.0107 0.0094 ## 6 0.0104 0.0045 0.0014 0.0038 0.0013 0.0089 0.0057 0.0027 0.0051 0.0062 ## Class ## 1 R ## 2 R ## 3 R ## 4 R ## 5 R ## 6 R # Soybean dataset data(Soybean) dim(Soybean) ## [1] 683 36 levels(Soybean$Class) ## [1] &quot;2-4-d-injury&quot; &quot;alternarialeaf-spot&quot; ## [3] &quot;anthracnose&quot; &quot;bacterial-blight&quot; ## [5] &quot;bacterial-pustule&quot; &quot;brown-spot&quot; ## [7] &quot;brown-stem-rot&quot; &quot;charcoal-rot&quot; ## [9] &quot;cyst-nematode&quot; &quot;diaporthe-pod-&amp;-stem-blight&quot; ## [11] &quot;diaporthe-stem-canker&quot; &quot;downy-mildew&quot; ## [13] &quot;frog-eye-leaf-spot&quot; &quot;herbicide-injury&quot; ## [15] &quot;phyllosticta-leaf-spot&quot; &quot;phytophthora-rot&quot; ## [17] &quot;powdery-mildew&quot; &quot;purple-seed-stain&quot; ## [19] &quot;rhizoctonia-root-rot&quot; head(Soybean) ## Class date plant.stand precip temp hail crop.hist ## 1 diaporthe-stem-canker 6 0 2 1 0 1 ## 2 diaporthe-stem-canker 4 0 2 1 0 2 ## 3 diaporthe-stem-canker 3 0 2 1 0 1 ## 4 diaporthe-stem-canker 3 0 2 1 0 1 ## 5 diaporthe-stem-canker 6 0 2 1 0 2 ## 6 diaporthe-stem-canker 5 0 2 1 0 3 ## area.dam sever seed.tmt germ plant.growth leaves leaf.halo leaf.marg ## 1 1 1 0 0 1 1 0 2 ## 2 0 2 1 1 1 1 0 2 ## 3 0 2 1 2 1 1 0 2 ## 4 0 2 0 1 1 1 0 2 ## 5 0 1 0 2 1 1 0 2 ## 6 0 1 0 1 1 1 0 2 ## leaf.size leaf.shread leaf.malf leaf.mild stem lodging stem.cankers ## 1 2 0 0 0 1 1 3 ## 2 2 0 0 0 1 0 3 ## 3 2 0 0 0 1 0 3 ## 4 2 0 0 0 1 0 3 ## 5 2 0 0 0 1 0 3 ## 6 2 0 0 0 1 0 3 ## canker.lesion fruiting.bodies ext.decay mycelium int.discolor sclerotia ## 1 1 1 1 0 0 0 ## 2 1 1 1 0 0 0 ## 3 0 1 1 0 0 0 ## 4 0 1 1 0 0 0 ## 5 1 1 1 0 0 0 ## 6 0 1 1 0 0 0 ## fruit.pods fruit.spots seed mold.growth seed.discolor seed.size ## 1 0 4 0 0 0 0 ## 2 0 4 0 0 0 0 ## 3 0 4 0 0 0 0 ## 4 0 4 0 0 0 0 ## 5 0 4 0 0 0 0 ## 6 0 4 0 0 0 0 ## shriveling roots ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 # # Datasets from the AppliedPredictiveModeling library # load the library library(AppliedPredictiveModeling) # list the contents of the library #library(help = &quot;AppliedPredictiveModeling&quot;) # Abalone Data data(abalone) dim(abalone) ## [1] 4177 9 head(abalone) ## Type LongestShell Diameter Height WholeWeight ShuckedWeight ## 1 M 0.455 0.365 0.095 0.5140 0.2245 ## 2 M 0.350 0.265 0.090 0.2255 0.0995 ## 3 F 0.530 0.420 0.135 0.6770 0.2565 ## 4 M 0.440 0.365 0.125 0.5160 0.2155 ## 5 I 0.330 0.255 0.080 0.2050 0.0895 ## 6 I 0.425 0.300 0.095 0.3515 0.1410 ## VisceraWeight ShellWeight Rings ## 1 0.1010 0.150 15 ## 2 0.0485 0.070 7 ## 3 0.1415 0.210 9 ## 4 0.1140 0.155 10 ## 5 0.0395 0.055 7 ## 6 0.0775 0.120 8 # Datasets from the dataset library # list the contents of the library #library(help = &quot;datasets&quot;) library(datasets) # list all available datasets in all loaded libraries # Iris flowers datasets data(iris) dim(iris) ## [1] 150 5 levels(iris$Species) ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # Longley&#39;s Economic Regression Data data(longley) dim(longley) ## [1] 16 7 head(longley) ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234.289 235.6 159.0 107.608 1947 60.323 ## 1948 88.5 259.426 232.5 145.6 108.632 1948 61.122 ## 1949 88.2 258.054 368.2 161.6 109.773 1949 60.171 ## 1950 89.5 284.599 335.1 165.0 110.929 1950 61.187 ## 1951 96.2 328.975 209.9 309.9 112.075 1951 63.221 ## 1952 98.1 346.999 193.2 359.4 113.270 1952 63.639 # Load data from a CSV file in the local directory # define the filename # filename &lt;- &quot;/Users/tymen/RHome/machine_learning_mastery_with_r/code/1-AnalyzeData/1-LoadData/iris.csv&quot; # # load the CSV file from the local directory # dataset &lt;- read_csv(filename) # preview the first 5 rows #head(dataset) # Load CSV From a URL # urlfile &lt;-&#39;http://www.sigkdd.org/kddcup/index.php?section=1998&amp;method=data&#39; # downloaded &lt;- getURL(urlfile, ssl.verifypeer=FALSE) # connection &lt;- textConnection(downloaded) # dataset &lt;- read_csv(connection) # head(dataset) # load the library library(RCurl) # specify the URL for the Iris data CSV urlfile &lt;-&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#39; # download the file downloaded &lt;- getURL(urlfile, ssl.verifypeer=FALSE) # treat the text data as a steam so we can read from it connection &lt;- textConnection(downloaded) # parse the downloaded data as CSV dataset &lt;- read.csv(connection, header=FALSE) # preview the first 5 rows head(dataset) ## V1 V2 V3 V4 V5 ## 1 5.1 3.5 1.4 0.2 Iris-setosa ## 2 4.9 3.0 1.4 0.2 Iris-setosa ## 3 4.7 3.2 1.3 0.2 Iris-setosa ## 4 4.6 3.1 1.5 0.2 Iris-setosa ## 5 5.0 3.6 1.4 0.2 Iris-setosa ## 6 5.4 3.9 1.7 0.4 Iris-setosa "],
["prepare-problem-1.html", "Chapter 2 Prepare Problem 2.1 Descriptive statistics 2.2 Data visualizations Univariate 2.3 Data visualizations projection", " Chapter 2 Prepare Problem 2.1 Descriptive statistics # load the library library(mlbench) # load the dataset data(PimaIndiansDiabetes) # display first 20 rows of data head(PimaIndiansDiabetes, n=20) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 0 33.6 0.627 50 pos ## 2 1 85 66 29 0 26.6 0.351 31 neg ## 3 8 183 64 0 0 23.3 0.672 32 pos ## 4 1 89 66 23 94 28.1 0.167 21 neg ## 5 0 137 40 35 168 43.1 2.288 33 pos ## 6 5 116 74 0 0 25.6 0.201 30 neg ## 7 3 78 50 32 88 31.0 0.248 26 pos ## 8 10 115 0 0 0 35.3 0.134 29 neg ## 9 2 197 70 45 543 30.5 0.158 53 pos ## 10 8 125 96 0 0 0.0 0.232 54 pos ## 11 4 110 92 0 0 37.6 0.191 30 neg ## 12 10 168 74 0 0 38.0 0.537 34 pos ## 13 10 139 80 0 0 27.1 1.441 57 neg ## 14 1 189 60 23 846 30.1 0.398 59 pos ## 15 5 166 72 19 175 25.8 0.587 51 pos ## 16 7 100 0 0 0 30.0 0.484 32 pos ## 17 0 118 84 47 230 45.8 0.551 31 pos ## 18 7 107 74 0 0 29.6 0.254 31 pos ## 19 1 103 30 38 83 43.3 0.183 33 neg ## 20 1 115 70 30 96 34.6 0.529 32 pos library(datasets) # load the iris dataset data(iris) # summarize the dataset summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## data(BostonHousing) # list types for each attribute sapply(BostonHousing, class) ## crim zn indus chas nox rm age ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;factor&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## dis rad tax ptratio b lstat medv ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; # Class Distribution # load the libraries library(mlbench) # load the dataset data(PimaIndiansDiabetes) # distribution of class variable y &lt;- PimaIndiansDiabetes$diabetes cbind(freq=table(y), percentage=prop.table(table(y))*100) ## freq percentage ## neg 500 65.10417 ## pos 268 34.89583 # Dimensions of your dataset dim(PimaIndiansDiabetes) ## [1] 768 9 # summarize the dataset summary(PimaIndiansDiabetes) ## pregnant glucose pressure triceps ## Min. : 0.000 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 1.000 1st Qu.: 99.0 1st Qu.: 62.00 1st Qu.: 0.00 ## Median : 3.000 Median :117.0 Median : 72.00 Median :23.00 ## Mean : 3.845 Mean :120.9 Mean : 69.11 Mean :20.54 ## 3rd Qu.: 6.000 3rd Qu.:140.2 3rd Qu.: 80.00 3rd Qu.:32.00 ## Max. :17.000 Max. :199.0 Max. :122.00 Max. :99.00 ## insulin mass pedigree age ## Min. : 0.0 Min. : 0.00 Min. :0.0780 Min. :21.00 ## 1st Qu.: 0.0 1st Qu.:27.30 1st Qu.:0.2437 1st Qu.:24.00 ## Median : 30.5 Median :32.00 Median :0.3725 Median :29.00 ## Mean : 79.8 Mean :31.99 Mean :0.4719 Mean :33.24 ## 3rd Qu.:127.2 3rd Qu.:36.60 3rd Qu.:0.6262 3rd Qu.:41.00 ## Max. :846.0 Max. :67.10 Max. :2.4200 Max. :81.00 ## diabetes ## neg:500 ## pos:268 ## ## ## ## # Calculate Skewness # load the dataset data(PimaIndiansDiabetes) # calculate skewness for each variable #skew &lt;- apply(PimaIndiansDiabetes[,1:8], 2, skewness) # display skewness, larger/smaller deviations from 0 show more skew #print(skew) # calculate standard deviation for all attributes sapply(PimaIndiansDiabetes[,1:8], sd) ## pregnant glucose pressure triceps insulin mass ## 3.3695781 31.9726182 19.3558072 15.9522176 115.2440024 7.8841603 ## pedigree age ## 0.3313286 11.7602315 # Pair-wise correlations using pearson spearman coefficients correlations &lt;- cor(PimaIndiansDiabetes[,1:8], method=&quot;spearman&quot;) # display the correlation matrix print(correlations) ## pregnant glucose pressure triceps insulin ## pregnant 1.0000000000 0.13073352 0.185126732 -0.08522231 -0.126722724 ## glucose 0.1307335241 1.00000000 0.235190613 0.06002215 0.213205805 ## pressure 0.1851267321 0.23519061 1.000000000 0.12648587 -0.006770572 ## triceps -0.0852223076 0.06002215 0.126485871 1.00000000 0.541000137 ## insulin -0.1267227242 0.21320580 -0.006770572 0.54100014 1.000000000 ## mass 0.0001321469 0.23114119 0.292870430 0.44361451 0.192725681 ## pedigree -0.0432415012 0.09129336 0.030046335 0.18039048 0.221150492 ## age 0.6072163388 0.28504472 0.350894593 -0.06679492 -0.114212917 ## mass pedigree age ## pregnant 0.0001321469 -0.04324150 0.60721634 ## glucose 0.2311411943 0.09129336 0.28504472 ## pressure 0.2928704303 0.03004633 0.35089459 ## triceps 0.4436145083 0.18039048 -0.06679492 ## insulin 0.1927256806 0.22115049 -0.11421292 ## mass 1.0000000000 0.14119203 0.13118588 ## pedigree 0.1411920297 1.00000000 0.04290859 ## age 0.1311858805 0.04290859 1.00000000 2.2 Data visualizations Univariate # Univariate Histograms # load the data data(iris) # create histograms for each attribute par(mfrow=c(1,4)) for(i in 1:4) { hist(iris[,i], main=names(iris)[i]) } # Univariate Density Plots # load dataset data(iris) # create a panel of simpler density plots by attribute par(mfrow=c(1,4)) for(i in 1:4) { plot(density(iris[,i]), main=names(iris)[i]) } # Univarate Box And Whisker Plots # load dataset data(iris) # Create separate boxplots for each attribute par(mfrow=c(1,4)) for(i in 1:4) { boxplot(iris[,i], main=names(iris)[i]) } # Plot missing data # load libraries library(Amelia) library(mlbench) # load dataset data(Soybean) # create a missing map missmap(Soybean, col=c(&quot;black&quot;, &quot;grey&quot;), legend=FALSE) ## Data visualizations multivariate # Create a density plot for each variable-class combination. # load the library library(caret) # load the data data(iris) # density plots for each attribute by class value x &lt;- iris[,1:4] y &lt;- iris[,5] scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) featurePlot(x=x, y=y, plot=&quot;density&quot;, scales=scales) # Multivariate Scatterplot Matrix By Class # load the data data(iris) # pair-wise scatterplots colored by class pairs(Species~., data=iris, col=iris$Species) # Create a box and whisker plots for each variable organized by class. # load the iris dataset data(iris) # box and whisker plots for each attribute by class value x &lt;- iris[,1:4] y &lt;- iris[,5] featurePlot(x=x, y=y, plot=&quot;box&quot;) # Multivariate Scatterplot Matrix # load the data data(iris) # pair-wise scatterplots of all 4 attributes pairs(iris) # Correlation Plot # load library library(corrplot) # load the data data(iris) # calculate correlations correlations &lt;- cor(iris[,1:4]) # create correlation plot corrplot(correlations, method=&quot;circle&quot;) 2.3 Data visualizations projection # Self Organizing Map (Kohonen) # load the library library(&quot;kohonen&quot;) # load the dataset data(iris) # split input and output x &lt;- data.matrix(iris[,1:4]) y &lt;- iris[,5] # set the random seed for repetable results set.seed(7) # create a map of the x values iris_map &lt;- som(x, grid=somgrid(5, 5, &quot;hexagonal&quot;)) # plot the map plot(iris_map) # TODO label the map by class # Sammons Mapping # load library library(MASS) # load dataset data(iris) # remove duplicates clean &lt;- unique(iris) # split out numerical inputs x &lt;- data.matrix(clean[, 1:4]) # create a sammon mapping mapping &lt;- sammon(dist(x)) ## Initial stress : 0.00678 ## stress after 10 iters: 0.00404, magic = 0.500 ## stress after 12 iters: 0.00402 # plot mapping by class plot(mapping$points, type=&quot;n&quot;) text(mapping$points, labels=clean[,5]) # TODO colour dots by class # Principal Component Analysis # load the dataset data(iris) # separate numerical inputs x &lt;- data.matrix(iris[,1:4]) y &lt;- iris[,5] # calculate components components &lt;- prcomp(x, center=TRUE, scale=TRUE) # display components print(components) ## Standard deviations (1, .., p=4): ## [1] 1.7083611 0.9560494 0.3830886 0.1439265 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.5210659 -0.37741762 0.7195664 0.2612863 ## Sepal.Width -0.2693474 -0.92329566 -0.2443818 -0.1235096 ## Petal.Length 0.5804131 -0.02449161 -0.1421264 -0.8014492 ## Petal.Width 0.5648565 -0.06694199 -0.6342727 0.5235971 # summarize components summary(components) ## Importance of components%s: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.7084 0.9560 0.38309 0.14393 ## Proportion of Variance 0.7296 0.2285 0.03669 0.00518 ## Cumulative Proportion 0.7296 0.9581 0.99482 1.00000 # plot the components biplot(components) # Andrews Curves # load library library(andrews) # load dataset data(iris) # generate andres curves andrews(iris, clr=5, ymax=3) "],
["prepare-data.html", "Chapter 3 Prepare Data 3.1 Data Cleaning 3.2 Feature Selection 3.3 Data Transforms", " Chapter 3 Prepare Data 3.1 Data Cleaning # Remove rows with NA # load library library(mlbench) # load dataset data(BreastCancer) # summarize dimensions of dataset dim(BreastCancer) ## [1] 699 11 # Remove all incomplete rows dataset &lt;- BreastCancer[complete.cases(BreastCancer),] # summarize dimensions of resulting dataset dim(dataset) ## [1] 683 11 # Update Data Frame to Remove Outliers # load the libraries library(mlbench) # load the dataset data(PimaIndiansDiabetes) # calculate stats for pregnant (number of times pregnant) pregnant.mean &lt;- mean(PimaIndiansDiabetes$pregnant) pregnant.sd &lt;- sd(PimaIndiansDiabetes$pregnant) # max reasonable value is within 99.7% of the data (if Gaussian) pregnant.max &lt;- pregnant.mean + (3*pregnant.sd) # mark outlier pregnant values as N/A PimaIndiansDiabetes$pregnant[PimaIndiansDiabetes$pregnant&gt;pregnant.max] &lt;- NA # Remove rows with NA # load library library(mlbench) # load dataset data(BreastCancer) # summarize dimensions of dataset dim(BreastCancer) ## [1] 699 11 # Remove all incomplete rows dataset &lt;- BreastCancer[complete.cases(BreastCancer),] # summarize dimensions of resulting dataset dim(dataset) ## [1] 683 11 # Impute missing values # load the libraries library(mlbench) library(&quot;survival&quot;, lib.loc=&quot;/usr/local/Cellar/r/3.4.1_1/lib/R/library&quot;) library(Hmisc) # load the dataset data(PimaIndiansDiabetes) # mark a pressure of 0 as N/A, it is impossible invalid &lt;- 0 PimaIndiansDiabetes$pressure[PimaIndiansDiabetes$pressure==invalid] &lt;- NA # impute missing pressure values using the mean #PimaIndiansDiabetes$pressure &lt;- with(PimaIndiansDiabetes, impute(pressure, mean)) # Remove Duplicate Instances # load the libraries library(mlbench) library(datasets) # load the dataset data(iris) dim(iris) ## [1] 150 5 # remove duplicates clean &lt;- unique(iris) dim(clean) ## [1] 149 5 # Rebalance a dataset using Synthetic Minority Over-sampling Technique (SMOTE) # load the libraries library(mlbench) #library(DMwR) # load the dataset data(PimaIndiansDiabetes) # display count of instances of each class (unbalanced) table(PimaIndiansDiabetes$diabetes) ## ## neg pos ## 500 268 # use SMOTE to created a &quot;more balance&quot; version of the dataset # balanced &lt;- SMOTE(diabetes~., PimaIndiansDiabetes, perc.over=300, perc.under=100) 3.2 Feature Selection # Identify and remove highly correlated features # load the libraries library(mlbench) library(&quot;caret&quot;, lib.loc=&quot;~/Library/R/3.4/library&quot;) # load the data data(PimaIndiansDiabetes) # calculate correlation matrix correlationMatrix &lt;- cor(PimaIndiansDiabetes[,1:8]) # find attributes that are highly corrected (ideally &gt;0.75) cutoff &lt;- 0.50 highlyCorrelated &lt;- findCorrelation(correlationMatrix, cutoff=cutoff) # create a new dataset without highly corrected features dataset &lt;- PimaIndiansDiabetes[,-highlyCorrelated] # Use RFE and to select features # load the library library(mlbench) library(caret) # load the data data(Sonar) # define the control using a random forest selection function control &lt;- rfeControl(functions=rfFuncs, method=&quot;cv&quot;, number=10) # run the RFE algorithm x &lt;- Sonar[,1:60] y &lt;- Sonar[,61] sizes &lt;- c(10,20,30,40,50,60) results &lt;- rfe(x, y, sizes=sizes, rfeControl=control) # summarize the results print(results) ## ## Recursive feature selection ## ## Outer resampling method: Cross-Validated (10 fold) ## ## Resampling performance over subset size: ## ## Variables Accuracy Kappa AccuracySD KappaSD Selected ## 10 0.7873 0.5737 0.10830 0.2150 ## 20 0.8216 0.6397 0.10699 0.2154 ## 30 0.8356 0.6703 0.13012 0.2591 ## 40 0.8600 0.7171 0.10938 0.2205 ## 50 0.8652 0.7259 0.08836 0.1812 ## 60 0.8793 0.7551 0.08966 0.1819 * ## ## The top 5 variables (out of 60): ## V11, V12, V9, V10, V48 # list the chosen features #predictors(results) # plot accuracy versus the number of features plot(results, type=c(&quot;g&quot;, &quot;o&quot;)) # Rank features by their importance. # load the libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;cv&quot;, number=10) # train the model model &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lvq&quot;, preProcess=&quot;scale&quot;, trControl=control) # estimate variable importance importance &lt;- varImp(model, scale=FALSE) # summarize importance print(importance) ## ROC curve variable importance ## ## Importance ## glucose 0.7881 ## mass 0.6876 ## age 0.6869 ## pregnant 0.6195 ## pedigree 0.6062 ## pressure 0.5865 ## triceps 0.5536 ## insulin 0.5379 # plot importance plot(importance) 3.3 Data Transforms # Center attributes by subtracting the mean library(datasets) # load libraries library(&quot;caret&quot;, lib.loc=&quot;~/Library/R/3.4/library&quot;) # load the dataset data(iris) # summarize data summary(iris[,1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 # calculate the pre-process parameters from the dataset preprocessParams &lt;- preProcess(iris[,1:4], method=c(&quot;center&quot;)) # summarize transform parameters print(preprocessParams) ## Created from 150 samples and 4 variables ## ## Pre-processing: ## - centered (4) ## - ignored (0) # transform the dataset using the parameters transformed &lt;- predict(preprocessParams, iris[,1:4]) # summarize the transformed dataset summary(transformed) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :-1.54333 Min. :-1.05733 Min. :-2.758 Min. :-1.0993 ## 1st Qu.:-0.74333 1st Qu.:-0.25733 1st Qu.:-2.158 1st Qu.:-0.8993 ## Median :-0.04333 Median :-0.05733 Median : 0.592 Median : 0.1007 ## Mean : 0.00000 Mean : 0.00000 Mean : 0.000 Mean : 0.0000 ## 3rd Qu.: 0.55667 3rd Qu.: 0.24267 3rd Qu.: 1.342 3rd Qu.: 0.6007 ## Max. : 2.05667 Max. : 1.34267 Max. : 3.142 Max. : 1.3007 # Standardize numeric attributes so they have zero mean and unit variance. # load libraries library(caret) # load the dataset data(iris) # summarize data summary(iris[,1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 # calculate the pre-process parameters from the dataset preprocessParams &lt;- preProcess(iris[,1:4], method=c(&quot;center&quot;, &quot;scale&quot;)) # summarize transform parameters print(preprocessParams) ## Created from 150 samples and 4 variables ## ## Pre-processing: ## - centered (4) ## - ignored (0) ## - scaled (4) # transform the dataset using the parameters transformed &lt;- predict(preprocessParams, iris[,1:4]) # summarize the transformed dataset summary(transformed) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :-1.86378 Min. :-2.4258 Min. :-1.5623 Min. :-1.4422 ## 1st Qu.:-0.89767 1st Qu.:-0.5904 1st Qu.:-1.2225 1st Qu.:-1.1799 ## Median :-0.05233 Median :-0.1315 Median : 0.3354 Median : 0.1321 ## Mean : 0.00000 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 0.67225 3rd Qu.: 0.5567 3rd Qu.: 0.7602 3rd Qu.: 0.7880 ## Max. : 2.48370 Max. : 3.0805 Max. : 1.7799 Max. : 1.7064 # Scale attributes by dividing by standard deviation # load libraries library(caret) # load the dataset data(iris) # summarize data summary(iris[,1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 # calculate the pre-process parameters from the dataset preprocessParams &lt;- preProcess(iris[,1:4], method=c(&quot;scale&quot;)) preprocessParams &lt;- preProcess(iris[,1:4], method=c(&quot;range&quot;)) # summarize transform parameters print(preprocessParams) ## Created from 150 samples and 4 variables ## ## Pre-processing: ## - ignored (0) ## - re-scaling to [0, 1] (4) # transform the dataset using the parameters transformed &lt;- predict(preprocessParams, iris[,1:4]) # summarize the transformed dataset summary(transformed) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.00000 ## 1st Qu.:0.2222 1st Qu.:0.3333 1st Qu.:0.1017 1st Qu.:0.08333 ## Median :0.4167 Median :0.4167 Median :0.5678 Median :0.50000 ## Mean :0.4287 Mean :0.4406 Mean :0.4675 Mean :0.45806 ## 3rd Qu.:0.5833 3rd Qu.:0.5417 3rd Qu.:0.6949 3rd Qu.:0.70833 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.00000 # Normalize numeric attributes to the range [0,1] # load libraries library(caret) # load the dataset data(iris) # summarize data summary(iris[,1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 # calculate the pre-process parameters from the dataset preprocessParams &lt;- preProcess(iris[,1:4], method=c(&quot;range&quot;)) # summarize transform parameters print(preprocessParams) ## Created from 150 samples and 4 variables ## ## Pre-processing: ## - ignored (0) ## - re-scaling to [0, 1] (4) # transform the dataset using the parameters transformed &lt;- predict(preprocessParams, iris[,1:4]) # summarize the transformed dataset summary(transformed) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.00000 ## 1st Qu.:0.2222 1st Qu.:0.3333 1st Qu.:0.1017 1st Qu.:0.08333 ## Median :0.4167 Median :0.4167 Median :0.5678 Median :0.50000 ## Mean :0.4287 Mean :0.4406 Mean :0.4675 Mean :0.45806 ## 3rd Qu.:0.5833 3rd Qu.:0.5417 3rd Qu.:0.6949 3rd Qu.:0.70833 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.00000 # Box-Cox Transform (attributes must be numeric and &gt;0) # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # summarize pedigree and age summary(PimaIndiansDiabetes[,7:8]) ## pedigree age ## Min. :0.0780 Min. :21.00 ## 1st Qu.:0.2437 1st Qu.:24.00 ## Median :0.3725 Median :29.00 ## Mean :0.4719 Mean :33.24 ## 3rd Qu.:0.6262 3rd Qu.:41.00 ## Max. :2.4200 Max. :81.00 # calculate the pre-process parameters from the dataset preprocessParams &lt;- preProcess(PimaIndiansDiabetes[,7:8], method=c(&quot;BoxCox&quot;)) # summarize transform parameters print(preprocessParams) ## Created from 768 samples and 2 variables ## ## Pre-processing: ## - Box-Cox transformation (2) ## - ignored (0) ## ## Lambda estimates for Box-Cox transformation: ## -0.1, -1.1 # transform the dataset using the parameters transformed &lt;- predict(preprocessParams, PimaIndiansDiabetes[,7:8]) # summarize the transformed dataset (note pedigree and age) summary(transformed) ## pedigree age ## Min. :-2.5510 Min. :0.8772 ## 1st Qu.:-1.4116 1st Qu.:0.8815 ## Median :-0.9875 Median :0.8867 ## Mean :-0.9599 Mean :0.8874 ## 3rd Qu.:-0.4680 3rd Qu.:0.8938 ## Max. : 0.8838 Max. :0.9019 # Principal Component Analysis Pre-processing # load the libraries library(mlbench) # load the dataset data(iris) # summarize dataset summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## # calculate the pre-process parameters from the dataset preprocessParams &lt;- preProcess(iris, method=c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;)) # summarize transform parameters print(preprocessParams) ## Created from 150 samples and 5 variables ## ## Pre-processing: ## - centered (4) ## - ignored (1) ## - principal component signal extraction (4) ## - scaled (4) ## ## PCA needed 2 components to capture 95 percent of the variance # transform the dataset using the parameters transformed &lt;- predict(preprocessParams, iris) # summarize the transformed dataset summary(transformed) ## Species PC1 PC2 ## setosa :50 Min. :-2.7651 Min. :-2.67732 ## versicolor:50 1st Qu.:-2.0957 1st Qu.:-0.59205 ## virginica :50 Median : 0.4169 Median :-0.01744 ## Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 1.3385 3rd Qu.: 0.59649 ## Max. : 3.2996 Max. : 2.64521 # Independent Component Analysis Pre-processing # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # summarize dataset summary(PimaIndiansDiabetes[,1:8]) ## pregnant glucose pressure triceps ## Min. : 0.000 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 1.000 1st Qu.: 99.0 1st Qu.: 62.00 1st Qu.: 0.00 ## Median : 3.000 Median :117.0 Median : 72.00 Median :23.00 ## Mean : 3.845 Mean :120.9 Mean : 69.11 Mean :20.54 ## 3rd Qu.: 6.000 3rd Qu.:140.2 3rd Qu.: 80.00 3rd Qu.:32.00 ## Max. :17.000 Max. :199.0 Max. :122.00 Max. :99.00 ## insulin mass pedigree age ## Min. : 0.0 Min. : 0.00 Min. :0.0780 Min. :21.00 ## 1st Qu.: 0.0 1st Qu.:27.30 1st Qu.:0.2437 1st Qu.:24.00 ## Median : 30.5 Median :32.00 Median :0.3725 Median :29.00 ## Mean : 79.8 Mean :31.99 Mean :0.4719 Mean :33.24 ## 3rd Qu.:127.2 3rd Qu.:36.60 3rd Qu.:0.6262 3rd Qu.:41.00 ## Max. :846.0 Max. :67.10 Max. :2.4200 Max. :81.00 # calculate the pre-process parameters from the dataset preprocessParams &lt;- preProcess(PimaIndiansDiabetes[,1:8], method=c(&quot;center&quot;, &quot;scale&quot;, &quot;ica&quot;), n.comp=5) # summarize transform parameters print(preprocessParams) ## Created from 768 samples and 8 variables ## ## Pre-processing: ## - centered (8) ## - independent component signal extraction (8) ## - ignored (0) ## - scaled (8) ## ## ICA used 5 components # transform the dataset using the parameters transformed &lt;- predict(preprocessParams, PimaIndiansDiabetes[,1:8]) # summarize the transformed dataset summary(transformed) ## ICA1 ICA2 ICA3 ICA4 ## Min. :-3.2172 Min. :-1.5754 Min. :-2.93808 Min. :-4.17157 ## 1st Qu.:-0.6489 1st Qu.:-0.6823 1st Qu.:-0.72111 1st Qu.:-0.59502 ## Median :-0.1394 Median :-0.2596 Median :-0.07368 Median :-0.02422 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 Mean : 0.00000 ## 3rd Qu.: 0.4684 3rd Qu.: 0.4288 3rd Qu.: 0.73991 3rd Qu.: 0.48353 ## Max. : 5.5407 Max. : 6.0184 Max. : 2.37943 Max. : 4.89639 ## ICA5 ## Min. :-3.0701 ## 1st Qu.:-0.7723 ## Median : 0.2774 ## Mean : 0.0000 ## 3rd Qu.: 0.8408 ## Max. : 1.4165 # Yeo-Johnson Transform # load libraries library(mlbench) library(DMwR) # load the dataset data(PimaIndiansDiabetes) # display count of instances of each class (unbalanced) table(PimaIndiansDiabetes$diabetes) ## ## neg pos ## 500 268 # use SMOTE to created a &quot;more balance&quot; version of the dataset balanced &lt;- SMOTE(diabetes~., PimaIndiansDiabetes, perc.over=300, perc.under=100) "],
["evaluate-algorithms.html", "Chapter 4 04 Evaluate Algorithms 4.1 Spot-Check Algorithms 4.2 Linear Regression 4.3 Penalized Linear Regression 4.4 Linear Classification 4.5 NonLinear Classification", " Chapter 4 04 Evaluate Algorithms 4.1 Spot-Check Algorithms # Minimum examples of algorithm spot checks, both direct and with caret. # Linear Regression # lm direct use # load the library library(mlbench) # load data data(BostonHousing) # fit model fit &lt;- lm(medv~., BostonHousing) # summarize the fit print(fit) ## ## Call: ## lm(formula = medv ~ ., data = BostonHousing) ## ## Coefficients: ## (Intercept) crim zn indus chas1 ## 3.646e+01 -1.080e-01 4.642e-02 2.056e-02 2.687e+00 ## nox rm age dis rad ## -1.777e+01 3.810e+00 6.922e-04 -1.476e+00 3.060e-01 ## tax ptratio b lstat ## -1.233e-02 -9.527e-01 9.312e-03 -5.248e-01 # make predictions predictions &lt;- predict(fit, BostonHousing) # summarize accuracy mse &lt;- mean((BostonHousing$medv - predictions)^2) print(mse) ## [1] 21.89483 # lm in caret # load libraries library(caret) library(mlbench) # load dataset data(BostonHousing) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.lm &lt;- train(medv~., data=BostonHousing, method=&quot;lm&quot;, metric=&quot;RMSE&quot;, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=control) # summarize fit print(fit.lm) ## Linear Regression ## ## 506 samples ## 13 predictor ## ## Pre-processing: centered (13), scaled (13) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 405, 405, 403, 405, 406 ## Resampling results: ## ## RMSE Rsquared ## 4.822048 0.7322594 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE # Logistic Regression # glm direct use # load the library library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # fit model fit &lt;- glm(diabetes~., data=PimaIndiansDiabetes, family=binomial(link=&#39;logit&#39;)) # summarize the fit print(fit) ## ## Call: glm(formula = diabetes ~ ., family = binomial(link = &quot;logit&quot;), ## data = PimaIndiansDiabetes) ## ## Coefficients: ## (Intercept) pregnant glucose pressure triceps ## -8.404696 0.123182 0.035164 -0.013296 0.000619 ## insulin mass pedigree age ## -0.001192 0.089701 0.945180 0.014869 ## ## Degrees of Freedom: 767 Total (i.e. Null); 759 Residual ## Null Deviance: 993.5 ## Residual Deviance: 723.4 AIC: 741.4 # make predictions probabilities &lt;- predict(fit, PimaIndiansDiabetes[,1:8], type=&#39;response&#39;) predictions &lt;- ifelse(probabilities &gt; 0.5,&#39;pos&#39;,&#39;neg&#39;) # summarize accuracy table(predictions, PimaIndiansDiabetes$diabetes) ## ## predictions neg pos ## neg 445 112 ## pos 55 156 # glm in caret # load libraries library(caret) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.glm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;glm&quot;, metric=&quot;Accuracy&quot;, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=control) # summarize fit print(fit.glm) ## Generalized Linear Model ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## Pre-processing: centered (8), scaled (8) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results: ## ## Accuracy Kappa ## 0.7695442 0.4656824 # Linear Discriminant Analysis # lda direct # load the libraries library(MASS) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # fit model fit &lt;- lda(diabetes~., data=PimaIndiansDiabetes) # summarize the fit print(fit) ## Call: ## lda(diabetes ~ ., data = PimaIndiansDiabetes) ## ## Prior probabilities of groups: ## neg pos ## 0.6510417 0.3489583 ## ## Group means: ## pregnant glucose pressure triceps insulin mass pedigree ## neg 3.298000 109.9800 68.18400 19.66400 68.7920 30.30420 0.429734 ## pos 4.865672 141.2575 70.82463 22.16418 100.3358 35.14254 0.550500 ## age ## neg 31.19000 ## pos 37.06716 ## ## Coefficients of linear discriminants: ## LD1 ## pregnant 0.0938638298 ## glucose 0.0269863520 ## pressure -0.0106293929 ## triceps 0.0007043468 ## insulin -0.0008229296 ## mass 0.0603702056 ## pedigree 0.6711517147 ## age 0.0119490869 # make predictions predictions &lt;- predict(fit, PimaIndiansDiabetes[,1:8])$class # summarize accuracy table(predictions, PimaIndiansDiabetes$diabetes) ## ## predictions neg pos ## neg 446 112 ## pos 54 156 # lda in caret # load libraries library(caret) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, metric=&quot;Accuracy&quot;, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=control) # summarize fit print(fit.lda) ## Linear Discriminant Analysis ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## Pre-processing: centered (8), scaled (8) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results: ## ## Accuracy Kappa ## 0.7695527 0.4665998 # Regularized Regression # glmnet direct classification # load the library library(glmnet) library(mlbench) # load data data(PimaIndiansDiabetes) x &lt;- as.matrix(PimaIndiansDiabetes[,1:8]) y &lt;- as.matrix(PimaIndiansDiabetes[,9]) # fit model fit &lt;- glmnet(x, y, family=&quot;binomial&quot;, alpha=0.5, lambda=0.001) # summarize the fit print(fit) ## ## Call: glmnet(x = x, y = y, family = &quot;binomial&quot;, alpha = 0.5, lambda = 0.001) ## ## Df %Dev Lambda ## [1,] 8 0.2718 0.001 # make predictions predictions &lt;- predict(fit, x, type=&quot;class&quot;) # summarize accuracy table(predictions, PimaIndiansDiabetes$diabetes) ## ## predictions neg pos ## neg 446 112 ## pos 54 156 # glmnet direct regression # load the libraries library(glmnet) library(mlbench) # load data data(BostonHousing) BostonHousing$chas &lt;- as.numeric(as.character(BostonHousing$chas)) x &lt;- as.matrix(BostonHousing[,1:13]) y &lt;- as.matrix(BostonHousing[,14]) # fit model fit &lt;- glmnet(x, y, family=&quot;gaussian&quot;, alpha=0.5, lambda=0.001) # summarize the fit print(fit) ## ## Call: glmnet(x = x, y = y, family = &quot;gaussian&quot;, alpha = 0.5, lambda = 0.001) ## ## Df %Dev Lambda ## [1,] 13 0.7406 0.001 # make predictions predictions &lt;- predict(fit, x, type=&quot;link&quot;) # summarize accuracy mse &lt;- mean((y - predictions)^2) print(mse) ## [1] 21.89497 # glmnet in caret classification # load libraries library(caret) library(mlbench) library(glmnet) # Load the dataset data(PimaIndiansDiabetes) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.glmnet &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;glmnet&quot;, metric=&quot;Accuracy&quot;, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=control) # summarize fit print(fit.glmnet) ## glmnet ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## Pre-processing: centered (8), scaled (8) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results across tuning parameters: ## ## alpha lambda Accuracy Kappa ## 0.10 0.0004447834 0.7708514 0.4691039 ## 0.10 0.0044478343 0.7695442 0.4656452 ## 0.10 0.0444783425 0.7682285 0.4550876 ## 0.55 0.0004447834 0.7708514 0.4691039 ## 0.55 0.0044478343 0.7721416 0.4716536 ## 0.55 0.0444783425 0.7682370 0.4512513 ## 1.00 0.0004447834 0.7708429 0.4691785 ## 1.00 0.0044478343 0.7708344 0.4682322 ## 1.00 0.0444783425 0.7629997 0.4367982 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were alpha = 0.55 and lambda ## = 0.004447834. # glmnet in caret regression # load libraries library(caret) library(mlbench) library(glmnet) # Load the dataset data(BostonHousing) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.glmnet &lt;- train(medv~., data=BostonHousing, method=&quot;glmnet&quot;, metric=&quot;RMSE&quot;, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=control) # summarize fit print(fit.glmnet) ## glmnet ## ## 506 samples ## 13 predictor ## ## Pre-processing: centered (13), scaled (13) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 405, 405, 403, 405, 406 ## Resampling results across tuning parameters: ## ## alpha lambda RMSE Rsquared ## 0.10 0.01355531 4.821193 0.7323840 ## 0.10 0.13555307 4.823737 0.7321552 ## 0.10 1.35553073 5.009682 0.7157754 ## 0.55 0.01355531 4.821598 0.7322956 ## 0.55 0.13555307 4.855127 0.7289719 ## 0.55 1.35553073 5.330445 0.6844584 ## 1.00 0.01355531 4.821542 0.7322860 ## 1.00 0.13555307 4.919659 0.7219752 ## 1.00 1.35553073 5.517426 0.6741391 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 0.1 and lambda ## = 0.01355531. # k-Nearest Neighbors # knn direct classification # load the libraries library(caret) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # fit model fit &lt;- knn3(diabetes~., data=PimaIndiansDiabetes, k=3) # summarize the fit print(fit) ## 3-nearest neighbor classification model ## Training set class distribution: ## ## neg pos ## 500 268 # make predictions predictions &lt;- predict(fit, PimaIndiansDiabetes[,1:8], type=&quot;class&quot;) # summarize accuracy table(predictions, PimaIndiansDiabetes$diabetes) ## ## predictions neg pos ## neg 459 67 ## pos 41 201 # knn direct regression # load the libraries library(caret) library(mlbench) # load data data(BostonHousing) BostonHousing$chas &lt;- as.numeric(as.character(BostonHousing$chas)) x &lt;- as.matrix(BostonHousing[,1:13]) y &lt;- as.matrix(BostonHousing[,14]) # fit model fit &lt;- knnreg(x, y, k=3) # summarize the fit print(fit) ## 3-nearest neighbor regression model # make predictions predictions &lt;- predict(fit, x) # summarize accuracy mse &lt;- mean((BostonHousing$medv - predictions)^2) print(mse) ## [1] 17.9939 # knn in caret classification # load libraries library(caret) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, metric=&quot;Accuracy&quot;, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=control) # summarize fit print(fit.knn) ## k-Nearest Neighbors ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## Pre-processing: centered (8), scaled (8) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.7096426 0.3407639 ## 7 0.7200407 0.3649273 ## 9 0.7239453 0.3671314 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 9. # knn in caret regression # load libraries library(caret) data(BostonHousing) # Load the dataset data(BostonHousing) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.knn &lt;- train(medv~., data=BostonHousing, method=&quot;knn&quot;, metric=&quot;RMSE&quot;, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=control) # summarize fit print(fit.knn) ## k-Nearest Neighbors ## ## 506 samples ## 13 predictor ## ## Pre-processing: centered (13), scaled (13) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 405, 405, 403, 405, 406 ## Resampling results across tuning parameters: ## ## k RMSE Rsquared ## 5 4.403596 0.7854028 ## 7 4.563880 0.7714339 ## 9 4.686272 0.7644349 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 5. # Naive Bayes # naive bayes direct # load the libraries library(e1071) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # fit model fit &lt;- naiveBayes(diabetes~., data=PimaIndiansDiabetes) # summarize the fit print(fit) ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## neg pos ## 0.6510417 0.3489583 ## ## Conditional probabilities: ## pregnant ## Y [,1] [,2] ## neg 3.298000 3.017185 ## pos 4.865672 3.741239 ## ## glucose ## Y [,1] [,2] ## neg 109.9800 26.14120 ## pos 141.2575 31.93962 ## ## pressure ## Y [,1] [,2] ## neg 68.18400 18.06308 ## pos 70.82463 21.49181 ## ## triceps ## Y [,1] [,2] ## neg 19.66400 14.88995 ## pos 22.16418 17.67971 ## ## insulin ## Y [,1] [,2] ## neg 68.7920 98.86529 ## pos 100.3358 138.68912 ## ## mass ## Y [,1] [,2] ## neg 30.30420 7.689855 ## pos 35.14254 7.262967 ## ## pedigree ## Y [,1] [,2] ## neg 0.429734 0.2990853 ## pos 0.550500 0.3723545 ## ## age ## Y [,1] [,2] ## neg 31.19000 11.66765 ## pos 37.06716 10.96825 # make predictions predictions &lt;- predict(fit, PimaIndiansDiabetes[,1:8]) # summarize accuracy table(predictions, PimaIndiansDiabetes$diabetes) ## ## predictions neg pos ## neg 421 104 ## pos 79 164 # naive bayes in caret # load libraries library(caret) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.nb &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;nb&quot;, metric=&quot;Accuracy&quot;, trControl=control) # summarize fit print(fit.nb) ## Naive Bayes ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results across tuning parameters: ## ## usekernel Accuracy Kappa ## FALSE 0.7500042 0.4350123 ## TRUE 0.7460997 0.4236266 ## ## Tuning parameter &#39;fL&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;adjust&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were fL = 0, usekernel = FALSE ## and adjust = 1. # Support Vector Machine # SVM direct classification # load the libraries library(kernlab) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # fit model fit &lt;- ksvm(diabetes~., data=PimaIndiansDiabetes, kernel=&quot;rbfdot&quot;) # summarize the fit print(fit) ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.138462840445792 ## ## Number of Support Vectors : 441 ## ## Objective Function Value : -348.8559 ## Training error : 0.166667 # make predictions predictions &lt;- predict(fit, PimaIndiansDiabetes[,1:8], type=&quot;response&quot;) # summarize accuracy table(predictions, PimaIndiansDiabetes$diabetes) ## ## predictions neg pos ## neg 465 93 ## pos 35 175 # SVM direct regression # load the libraries library(kernlab) library(mlbench) # load data data(BostonHousing) # fit model fit &lt;- ksvm(medv~., BostonHousing, kernel=&quot;rbfdot&quot;) # summarize the fit print(fit) ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: eps-svr (regression) ## parameter : epsilon = 0.1 cost C = 1 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.114758032948535 ## ## Number of Support Vectors : 327 ## ## Objective Function Value : -74.5549 ## Training error : 0.087933 # make predictions predictions &lt;- predict(fit, BostonHousing) # summarize accuracy mse &lt;- mean((BostonHousing$medv - predictions)^2) print(mse) ## [1] 7.437927 # svmRadial in caret classification # load libraries library(caret) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.svmRadial &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, metric=&quot;Accuracy&quot;, trControl=control) # summarize fit print(fit.svmRadial) ## Support Vector Machines with Radial Basis Function Kernel ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results across tuning parameters: ## ## C Accuracy Kappa ## 0.25 0.7603684 0.4415912 ## 0.50 0.7551906 0.4322310 ## 1.00 0.7604278 0.4458565 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.1178216 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.1178216 and C = 1. # svmRadial in caret regression # load libraries library(caret) library(mlbench) # Load the dataset data(BostonHousing) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.svmRadial &lt;- train(medv~., data=BostonHousing, method=&quot;svmRadial&quot;, metric=&quot;RMSE&quot;, trControl=control) # summarize fit print(fit.svmRadial) ## Support Vector Machines with Radial Basis Function Kernel ## ## 506 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 405, 405, 403, 405, 406 ## Resampling results across tuning parameters: ## ## C RMSE Rsquared ## 0.25 4.818004 0.7564430 ## 0.50 4.279601 0.7985729 ## 1.00 3.820843 0.8337146 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.114971 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were sigma = 0.114971 and C = 1. # Classification and Regression Trees # CART direct classification # load the libraries library(rpart) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # fit model fit &lt;- rpart(diabetes~., data=PimaIndiansDiabetes) # summarize the fit print(fit) ## n= 768 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 768 268 neg (0.65104167 0.34895833) ## 2) glucose&lt; 127.5 485 94 neg (0.80618557 0.19381443) ## 4) age&lt; 28.5 271 23 neg (0.91512915 0.08487085) * ## 5) age&gt;=28.5 214 71 neg (0.66822430 0.33177570) ## 10) mass&lt; 26.35 41 2 neg (0.95121951 0.04878049) * ## 11) mass&gt;=26.35 173 69 neg (0.60115607 0.39884393) ## 22) glucose&lt; 99.5 55 10 neg (0.81818182 0.18181818) * ## 23) glucose&gt;=99.5 118 59 neg (0.50000000 0.50000000) ## 46) pedigree&lt; 0.561 84 34 neg (0.59523810 0.40476190) ## 92) pedigree&lt; 0.2 21 4 neg (0.80952381 0.19047619) * ## 93) pedigree&gt;=0.2 63 30 neg (0.52380952 0.47619048) ## 186) pregnant&gt;=1.5 52 21 neg (0.59615385 0.40384615) ## 372) pressure&gt;=67 40 12 neg (0.70000000 0.30000000) * ## 373) pressure&lt; 67 12 3 pos (0.25000000 0.75000000) * ## 187) pregnant&lt; 1.5 11 2 pos (0.18181818 0.81818182) * ## 47) pedigree&gt;=0.561 34 9 pos (0.26470588 0.73529412) * ## 3) glucose&gt;=127.5 283 109 pos (0.38515901 0.61484099) ## 6) mass&lt; 29.95 76 24 neg (0.68421053 0.31578947) ## 12) glucose&lt; 145.5 41 6 neg (0.85365854 0.14634146) * ## 13) glucose&gt;=145.5 35 17 pos (0.48571429 0.51428571) ## 26) insulin&lt; 14.5 21 8 neg (0.61904762 0.38095238) * ## 27) insulin&gt;=14.5 14 4 pos (0.28571429 0.71428571) * ## 7) mass&gt;=29.95 207 57 pos (0.27536232 0.72463768) ## 14) glucose&lt; 157.5 115 45 pos (0.39130435 0.60869565) ## 28) age&lt; 30.5 50 23 neg (0.54000000 0.46000000) ## 56) pressure&gt;=61 40 13 neg (0.67500000 0.32500000) ## 112) mass&lt; 41.8 31 7 neg (0.77419355 0.22580645) * ## 113) mass&gt;=41.8 9 3 pos (0.33333333 0.66666667) * ## 57) pressure&lt; 61 10 0 pos (0.00000000 1.00000000) * ## 29) age&gt;=30.5 65 18 pos (0.27692308 0.72307692) * ## 15) glucose&gt;=157.5 92 12 pos (0.13043478 0.86956522) * # make predictions predictions &lt;- predict(fit, PimaIndiansDiabetes[,1:8], type=&quot;class&quot;) # summarize accuracy table(predictions, PimaIndiansDiabetes$diabetes) ## ## predictions neg pos ## neg 449 72 ## pos 51 196 # CART direct regression # load the libraries library(rpart) library(mlbench) # load data data(BostonHousing) # fit model fit &lt;- rpart(medv~., data=BostonHousing, control=rpart.control(minsplit=5)) # summarize the fit print(fit) ## n= 506 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 506 42716.3000 22.53281 ## 2) rm&lt; 6.941 430 17317.3200 19.93372 ## 4) lstat&gt;=14.4 175 3373.2510 14.95600 ## 8) crim&gt;=6.99237 74 1085.9050 11.97838 * ## 9) crim&lt; 6.99237 101 1150.5370 17.13762 * ## 5) lstat&lt; 14.4 255 6632.2170 23.34980 ## 10) dis&gt;=1.38485 250 3721.1630 22.90520 ## 20) rm&lt; 6.543 195 1636.0670 21.62974 * ## 21) rm&gt;=6.543 55 643.1691 27.42727 * ## 11) dis&lt; 1.38485 5 390.7280 45.58000 * ## 3) rm&gt;=6.941 76 6059.4190 37.23816 ## 6) rm&lt; 7.437 46 1899.6120 32.11304 ## 12) crim&gt;=7.393425 3 27.9200 14.40000 * ## 13) crim&lt; 7.393425 43 864.7674 33.34884 * ## 7) rm&gt;=7.437 30 1098.8500 45.09667 ## 14) ptratio&gt;=18.3 3 223.8200 33.30000 * ## 15) ptratio&lt; 18.3 27 411.1585 46.40741 * # make predictions predictions &lt;- predict(fit, BostonHousing[,1:13]) # summarize accuracy mse &lt;- mean((BostonHousing$medv - predictions)^2) print(mse) ## [1] 12.71556 # rpart in caret classification # load libraries library(caret) library(mlbench) # Load the dataset data(PimaIndiansDiabetes) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=5) fit.rpart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, metric=&quot;Accuracy&quot;, trControl=control) # summarize fit print(fit.rpart) ## CART ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.01741294 0.7486631 0.4354181 ## 0.10447761 0.7395722 0.3922324 ## 0.24253731 0.7161956 0.3034238 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.01741294. # rpart in caret regression # load libraries library(caret) library(mlbench) # Load the dataset data(BostonHousing) # train set.seed(7) control &lt;- trainControl(method=&quot;cv&quot;, number=2) fit.rpart &lt;- train(medv~., data=BostonHousing, method=&quot;rpart&quot;, metric=&quot;RMSE&quot;, trControl=control) # summarize fit print(fit.rpart) ## CART ## ## 506 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (2 fold) ## Summary of sample sizes: 253, 253 ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared ## 0.07165784 5.983605 0.5774758 ## 0.17117244 7.173727 0.3939811 ## 0.45274420 7.173727 0.3939811 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0.07165784. 4.2 Linear Regression # Ordinary Least Squares Regression library(datasets) # load data data(longley) # fit model fit &lt;- lm(Employed~., longley) # summarize the fit print(fit) ## ## Call: ## lm(formula = Employed ~ ., data = longley) ## ## Coefficients: ## (Intercept) GNP.deflator GNP Unemployed Armed.Forces ## -3.482e+03 1.506e-02 -3.582e-02 -2.020e-02 -1.033e-02 ## Population Year ## -5.110e-02 1.829e+00 # make predictions predictions &lt;- predict(fit, longley) # summarize accuracy mse &lt;- mean((longley$Employed - predictions)^2) print(mse) ## [1] 0.0522765 # Stepwise Linear Regression # load data data(longley) # fit model base &lt;- lm(Employed~., longley) # summarize the fit summary(base) ## ## Call: ## lm(formula = Employed ~ ., data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.41011 -0.15767 -0.02816 0.10155 0.45539 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.482e+03 8.904e+02 -3.911 0.003560 ** ## GNP.deflator 1.506e-02 8.492e-02 0.177 0.863141 ## GNP -3.582e-02 3.349e-02 -1.070 0.312681 ## Unemployed -2.020e-02 4.884e-03 -4.136 0.002535 ** ## Armed.Forces -1.033e-02 2.143e-03 -4.822 0.000944 *** ## Population -5.110e-02 2.261e-01 -0.226 0.826212 ## Year 1.829e+00 4.555e-01 4.016 0.003037 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3049 on 9 degrees of freedom ## Multiple R-squared: 0.9955, Adjusted R-squared: 0.9925 ## F-statistic: 330.3 on 6 and 9 DF, p-value: 4.984e-10 # perform step-wise feature selection fit &lt;- step(base) ## Start: AIC=-33.22 ## Employed ~ GNP.deflator + GNP + Unemployed + Armed.Forces + Population + ## Year ## ## Df Sum of Sq RSS AIC ## - GNP.deflator 1 0.00292 0.83935 -35.163 ## - Population 1 0.00475 0.84117 -35.129 ## - GNP 1 0.10631 0.94273 -33.305 ## &lt;none&gt; 0.83642 -33.219 ## - Year 1 1.49881 2.33524 -18.792 ## - Unemployed 1 1.59014 2.42656 -18.178 ## - Armed.Forces 1 2.16091 2.99733 -14.798 ## ## Step: AIC=-35.16 ## Employed ~ GNP + Unemployed + Armed.Forces + Population + Year ## ## Df Sum of Sq RSS AIC ## - Population 1 0.01933 0.8587 -36.799 ## &lt;none&gt; 0.8393 -35.163 ## - GNP 1 0.14637 0.9857 -34.592 ## - Year 1 1.52725 2.3666 -20.578 ## - Unemployed 1 2.18989 3.0292 -16.628 ## - Armed.Forces 1 2.39752 3.2369 -15.568 ## ## Step: AIC=-36.8 ## Employed ~ GNP + Unemployed + Armed.Forces + Year ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 0.8587 -36.799 ## - GNP 1 0.4647 1.3234 -31.879 ## - Year 1 1.8980 2.7567 -20.137 ## - Armed.Forces 1 2.3806 3.2393 -17.556 ## - Unemployed 1 4.0491 4.9077 -10.908 # summarize the selected model print(fit) ## ## Call: ## lm(formula = Employed ~ GNP + Unemployed + Armed.Forces + Year, ## data = longley) ## ## Coefficients: ## (Intercept) GNP Unemployed Armed.Forces Year ## -3.599e+03 -4.019e-02 -2.088e-02 -1.015e-02 1.887e+00 # make predictions predictions &lt;- predict(fit, longley) # summarize accuracy mse &lt;- mean((longley$Employed - predictions)^2) print(mse) ## [1] 0.05366753 # Partial Least Squares Regression # load the package library(pls) # load data data(longley) # fit model fit &lt;- plsr(Employed~., data=longley, validation=&quot;CV&quot;) # summarize the fit print(fit) ## Partial least squares regression , fitted with the kernel algorithm. ## Cross-validated using 10 random segments. ## Call: ## plsr(formula = Employed ~ ., data = longley, validation = &quot;CV&quot;) # make predictions predictions &lt;- predict(fit, longley, ncomp=6) # summarize accuracy mse &lt;- mean((longley$Employed - predictions)^2) print(mse) ## [1] 0.0522765 # Principal Component Regression # load the package library(pls) # load data data(longley) # fit model fit &lt;- pcr(Employed~., data=longley, validation=&quot;CV&quot;) # summarize the fit print(fit) ## Principal component regression , fitted with the singular value decomposition algorithm. ## Cross-validated using 10 random segments. ## Call: ## pcr(formula = Employed ~ ., data = longley, validation = &quot;CV&quot;) # make predictions predictions &lt;- predict(fit, longley, ncomp=6) # summarize accuracy mse &lt;- mean((longley$Employed - predictions)^2) print(mse) ## [1] 0.0522765 4.3 Penalized Linear Regression # Elastic Net # load the package library(glmnet) # load data data(longley) x &lt;- as.matrix(longley[,1:6]) y &lt;- as.matrix(longley[,7]) # fit model fit &lt;- glmnet(x, y, family=&quot;gaussian&quot;, alpha=0.5, lambda=0.001) # summarize the fit print(fit) ## ## Call: glmnet(x = x, y = y, family = &quot;gaussian&quot;, alpha = 0.5, lambda = 0.001) ## ## Df %Dev Lambda ## [1,] 6 0.9949 0.001 # make predictions predictions &lt;- predict(fit, x, type=&quot;link&quot;) # summarize accuracy mse &lt;- mean((y - predictions)^2) print(mse) ## [1] 0.0590839 # Ridge Regression # load the package library(glmnet) # load data data(longley) x &lt;- as.matrix(longley[,1:6]) y &lt;- as.matrix(longley[,7]) # fit model fit &lt;- glmnet(x, y, family=&quot;gaussian&quot;, alpha=0, lambda=0.001) # summarize the fit print(fit) ## ## Call: glmnet(x = x, y = y, family = &quot;gaussian&quot;, alpha = 0, lambda = 0.001) ## ## Df %Dev Lambda ## [1,] 6 0.9949 0.001 # make predictions predictions &lt;- predict(fit, x, type=&quot;link&quot;) # summarize accuracy mse &lt;- mean((y - predictions)^2) print(mse) ## [1] 0.05919831 # Least Absolute Shrinkage and Selection Operator # load the package library(lars) # load data data(longley) x &lt;- as.matrix(longley[,1:6]) y &lt;- as.matrix(longley[,7]) # fit model fit &lt;- lars(x, y, type=&quot;lasso&quot;) # summarize the fit print(fit) ## ## Call: ## lars(x = x, y = y, type = &quot;lasso&quot;) ## R-squared: 0.995 ## Sequence of LASSO moves: ## GNP Unemployed Armed.Forces Year GNP Population GNP.deflator GNP ## Var 2 3 4 6 -2 5 1 2 ## Step 1 2 3 4 5 6 7 8 ## GNP.deflator GNP.deflator ## Var -1 1 ## Step 9 10 # select a step with a minimum error best_step &lt;- fit$df[which.min(fit$RSS)] # make predictions predictions &lt;- predict(fit, x, s=best_step, type=&quot;fit&quot;)$fit # summarize accuracy mse &lt;- mean((y - predictions)^2) print(mse) ## [1] 0.06400169 4.4 Linear Classification #Logistic Regression # Load the dataset data(PimaIndiansDiabetes) # fit model fit &lt;- glm(diabetes~., data=PimaIndiansDiabetes, family=binomial(link=&#39;logit&#39;)) # summarize the fit print(fit) ## ## Call: glm(formula = diabetes ~ ., family = binomial(link = &quot;logit&quot;), ## data = PimaIndiansDiabetes) ## ## Coefficients: ## (Intercept) pregnant glucose pressure triceps ## -8.404696 0.123182 0.035164 -0.013296 0.000619 ## insulin mass pedigree age ## -0.001192 0.089701 0.945180 0.014869 ## ## Degrees of Freedom: 767 Total (i.e. Null); 759 Residual ## Null Deviance: 993.5 ## Residual Deviance: 723.4 AIC: 741.4 # make predictions probabilities &lt;- predict(fit, PimaIndiansDiabetes[,1:8], type=&#39;response&#39;) predictions &lt;- ifelse(probabilities &gt; 0.5,&#39;pos&#39;,&#39;neg&#39;) # summarize accuracy table(predictions, PimaIndiansDiabetes$diabetes) ## ## predictions neg pos ## neg 445 112 ## pos 55 156 # Linear Discriminant Analysis # load the package library(MASS) data(iris) # fit model fit &lt;- lda(Species~., data=iris) # summarize the fit print(fit) ## Call: ## lda(Species ~ ., data = iris) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 5.006 3.428 1.462 0.246 ## versicolor 5.936 2.770 4.260 1.326 ## virginica 6.588 2.974 5.552 2.026 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 0.8293776 0.02410215 ## Sepal.Width 1.5344731 2.16452123 ## Petal.Length -2.2012117 -0.93192121 ## Petal.Width -2.8104603 2.83918785 ## ## Proportion of trace: ## LD1 LD2 ## 0.9912 0.0088 # make predictions predictions &lt;- predict(fit, iris[,1:4])$class # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 48 1 ## virginica 0 2 49 # Partial Least Squares Discriminant Analysis # load the package library(caret) data(iris) x &lt;- iris[,1:4] y &lt;- iris[,5] # fit model fit &lt;- plsda(x, y, probMethod=&quot;Bayes&quot;) # summarize the fit print(fit) ## Partial least squares classification, fitted with the kernel algorithm. ## Bayes rule was used to compute class probabilities. # make predictions predictions &lt;- predict(fit, iris[,1:4]) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 45 3 ## virginica 0 5 47 # Logistic Regression Multiclass # load the package library(VGAM) # load data data(iris) # fit model fit &lt;- vglm(Species~., family=multinomial, data=iris) # summarize the fit print(fit) ## ## Call: ## vglm(formula = Species ~ ., family = multinomial, data = iris) ## ## ## Coefficients: ## (Intercept):1 (Intercept):2 Sepal.Length:1 Sepal.Length:2 Sepal.Width:1 ## 35.360902 42.637804 9.637409 2.465220 12.358915 ## Sepal.Width:2 Petal.Length:1 Petal.Length:2 Petal.Width:1 Petal.Width:2 ## 6.680887 -23.214188 -9.429385 -34.101553 -18.286137 ## ## Degrees of Freedom: 300 Total; 290 Residual ## Residual deviance: 11.89855 ## Log-likelihood: -5.949274 ## ## This is a multinomial logit model with 3 levels # make predictions probabilities &lt;- predict(fit, iris[,1:4], type=&quot;response&quot;) predictions &lt;- apply(probabilities, 1, which.max) predictions[which(predictions==&quot;1&quot;)] &lt;- levels(iris$Species)[1] predictions[which(predictions==&quot;2&quot;)] &lt;- levels(iris$Species)[2] predictions[which(predictions==&quot;3&quot;)] &lt;- levels(iris$Species)[3] # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 49 1 ## virginica 0 1 49 4.5 NonLinear Classification # Random Forest # load the package library(randomForest) # load data data(iris) # fit model fit &lt;- randomForest(Species~., data=iris) # summarize the fit print(fit) ## ## Call: ## randomForest(formula = Species ~ ., data = iris) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 4.67% ## Confusion matrix: ## setosa versicolor virginica class.error ## setosa 50 0 0 0.00 ## versicolor 0 47 3 0.06 ## virginica 0 4 46 0.08 # make predictions predictions &lt;- predict(fit, iris[,1:4]) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 50 0 ## virginica 0 0 50 # k-Nearest Neighbors # load the package library(caret) data(iris) # fit model fit &lt;- knn3(Species~., data=iris, k=5) # summarize the fit print(fit) ## 5-nearest neighbor classification model ## Training set class distribution: ## ## setosa versicolor virginica ## 50 50 50 # make predictions predictions &lt;- predict(fit, iris[,1:4], type=&quot;class&quot;) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 2 ## virginica 0 3 48 # Naive Bayes # load the package library(e1071) data(iris) # fit model fit &lt;- naiveBayes(Species~., data=iris) # summarize the fit print(fit) ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Conditional probabilities: ## Sepal.Length ## Y [,1] [,2] ## setosa 5.006 0.3524897 ## versicolor 5.936 0.5161711 ## virginica 6.588 0.6358796 ## ## Sepal.Width ## Y [,1] [,2] ## setosa 3.428 0.3790644 ## versicolor 2.770 0.3137983 ## virginica 2.974 0.3224966 ## ## Petal.Length ## Y [,1] [,2] ## setosa 1.462 0.1736640 ## versicolor 4.260 0.4699110 ## virginica 5.552 0.5518947 ## ## Petal.Width ## Y [,1] [,2] ## setosa 0.246 0.1053856 ## versicolor 1.326 0.1977527 ## virginica 2.026 0.2746501 # make predictions predictions &lt;- predict(fit, iris[,1:4]) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 3 ## virginica 0 3 47 # Classification and Regression Trees # load the package library(rpart) # load data data(iris) # fit model fit &lt;- rpart(Species~., data=iris) # summarize the fit print(fit) ## n= 150 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333) ## 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&gt;=2.45 100 50 versicolor (0.00000000 0.50000000 0.50000000) ## 6) Petal.Width&lt; 1.75 54 5 versicolor (0.00000000 0.90740741 0.09259259) * ## 7) Petal.Width&gt;=1.75 46 1 virginica (0.00000000 0.02173913 0.97826087) * # make predictions predictions &lt;- predict(fit, iris[,1:4], type=&quot;class&quot;) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 49 5 ## virginica 0 1 45 # PART # load the package # library(RWeka) # # load data # data(iris) # # fit model # fit &lt;- PART(Species~., data=iris) # # summarize the fit # print(fit) # # make predictions # predictions &lt;- predict(fit, iris[,1:4]) # # summarize accuracy # table(predictions, iris$Species) # Classification and Regression Trees # load the package library(rpart) # load data data(iris) # fit model fit &lt;- rpart(Species~., data=iris) # summarize the fit print(fit) ## n= 150 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333) ## 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&gt;=2.45 100 50 versicolor (0.00000000 0.50000000 0.50000000) ## 6) Petal.Width&lt; 1.75 54 5 versicolor (0.00000000 0.90740741 0.09259259) * ## 7) Petal.Width&gt;=1.75 46 1 virginica (0.00000000 0.02173913 0.97826087) * # make predictions predictions &lt;- predict(fit, iris[,1:4], type=&quot;class&quot;) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 49 5 ## virginica 0 1 45 # C5.0 # load the package library(C50) # load data data(iris) # fit model fit &lt;- C5.0(Species~., data=iris, trials=10) # summarize the fit print(fit) ## ## Call: ## C5.0.formula(formula = Species ~ ., data = iris, trials = 10) ## ## Classification Tree ## Number of samples: 150 ## Number of predictors: 4 ## ## Number of boosting iterations: 10 ## Average tree size: 4.9 ## ## Non-standard options: attempt to group attributes # make predictions predictions &lt;- predict(fit, iris[,1:4]) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 50 0 ## virginica 0 0 50 # Flexible Discriminant Analysis # load the package library(mda) data(iris) # fit model fit &lt;- fda(Species~., data=iris) # summarize the fit print(fit) ## Call: ## fda(formula = Species ~ ., data = iris) ## ## Dimension: 2 ## ## Percent Between-Group Variance Explained: ## v1 v2 ## 99.12 100.00 ## ## Degrees of Freedom (per dimension): 5 ## ## Training Misclassification Error: 0.02 ( N = 150 ) # make predictions predictions &lt;- predict(fit, iris[,1:4]) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 48 1 ## virginica 0 2 49 # Gradient Boosted Machine # load the package library(gbm) # load data data(iris) # fit model fit &lt;- gbm(Species~., data=iris, distribution=&quot;multinomial&quot;) # summarize the fit print(fit) ## gbm(formula = Species ~ ., distribution = &quot;multinomial&quot;, data = iris) ## A gradient boosted model with multinomial loss function. ## 100 iterations were performed. ## There were 4 predictors of which 3 had non-zero influence. # make predictions probabilities &lt;- predict(fit, iris[,1:4], n.trees=1) predictions &lt;- colnames(probabilities)[apply(probabilities, 1, which.max)] # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 44 1 ## virginica 0 6 49 # Regularized Discriminant Analysis # load the package library(klaR) data(iris) # fit model fit &lt;- rda(Species~., data=iris, gamma=0.05, lambda=0.01) # summarize the fit print(fit) ## Call: ## rda(formula = Species ~ ., data = iris, gamma = 0.05, lambda = 0.01) ## ## Regularization parameters: ## gamma lambda ## 0.05 0.01 ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Misclassification rate: ## apparent: 2 % # make predictions predictions &lt;- predict(fit, iris[,1:4])$class # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 48 1 ## virginica 0 2 49 # Support Vector Machine # load the package library(kernlab) data(iris) # fit model fit &lt;- ksvm(Species~., data=iris) # summarize the fit print(fit) ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.79140603474701 ## ## Number of Support Vectors : 59 ## ## Objective Function Value : -4.5039 -5.0083 -20.3294 ## Training error : 0.026667 # make predictions predictions &lt;- predict(fit, iris[,1:4], type=&quot;response&quot;) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 48 2 ## virginica 0 2 48 # Feed Forward Neural Network # load the package library(nnet) data(iris) # fit model fit &lt;- nnet(Species~., data=iris, size=4, decay=0.0001, maxit=500) ## # weights: 35 ## initial value 170.333986 ## iter 10 value 69.831413 ## iter 20 value 69.449070 ## iter 30 value 69.430431 ## iter 40 value 11.186591 ## iter 50 value 6.379738 ## iter 60 value 6.190982 ## iter 70 value 6.124966 ## iter 80 value 6.105977 ## iter 90 value 6.093173 ## iter 100 value 6.064303 ## iter 110 value 6.062953 ## iter 120 value 6.057947 ## iter 130 value 6.054886 ## iter 140 value 6.049571 ## iter 150 value 5.907665 ## iter 160 value 3.947842 ## iter 170 value 2.617102 ## iter 180 value 1.941582 ## iter 190 value 1.513265 ## iter 200 value 1.154675 ## iter 210 value 0.995975 ## iter 220 value 0.971838 ## iter 230 value 0.928796 ## iter 240 value 0.902974 ## iter 250 value 0.901196 ## iter 260 value 0.899787 ## iter 270 value 0.897539 ## iter 280 value 0.896780 ## iter 290 value 0.895968 ## iter 300 value 0.895416 ## iter 310 value 0.893081 ## iter 320 value 0.872434 ## iter 330 value 0.818017 ## iter 340 value 0.771552 ## iter 350 value 0.714454 ## iter 360 value 0.681312 ## iter 370 value 0.614933 ## iter 380 value 0.567013 ## iter 390 value 0.546092 ## iter 400 value 0.528440 ## iter 410 value 0.501998 ## iter 420 value 0.484368 ## iter 430 value 0.452311 ## iter 440 value 0.423398 ## iter 450 value 0.415509 ## iter 460 value 0.392730 ## iter 470 value 0.380538 ## iter 480 value 0.370031 ## iter 490 value 0.360093 ## iter 500 value 0.352495 ## final value 0.352495 ## stopped after 500 iterations # summarize the fit print(fit) ## a 4-4-3 network with 35 weights ## inputs: Sepal.Length Sepal.Width Petal.Length Petal.Width ## output(s): Species ## options were - softmax modelling decay=1e-04 # make predictions predictions &lt;- predict(fit, iris[,1:4], type=&quot;class&quot;) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 50 0 ## virginica 0 0 50 # Quadratic Discriminant Analysis # load the package library(MASS) data(iris) # fit model fit &lt;- qda(Species~., data=iris) # summarize the fit print(fit) ## Call: ## qda(Species ~ ., data = iris) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 5.006 3.428 1.462 0.246 ## versicolor 5.936 2.770 4.260 1.326 ## virginica 6.588 2.974 5.552 2.026 # make predictions predictions &lt;- predict(fit, iris[,1:4])$class # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 48 1 ## virginica 0 2 49 # Mixture Discriminant Analysis # load the package library(mda) data(iris) # fit model fit &lt;- mda(Species~., data=iris) # summarize the fit print(fit) ## Call: ## mda(formula = Species ~ ., data = iris) ## ## Dimension: 4 ## ## Percent Between-Group Variance Explained: ## v1 v2 v3 v4 ## 96.14 98.60 99.90 100.00 ## ## Degrees of Freedom (per dimension): 5 ## ## Training Misclassification Error: 0.02 ( N = 150 ) ## ## Deviance: 15.045 # make predictions predictions &lt;- predict(fit, iris[,1:4]) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 48 1 ## virginica 0 2 49 # C4.5 # load the package # library(RWeka) # # load data # data(iris) # # fit model # fit &lt;- J48(Species~., data=iris) # # summarize the fit # print(fit) # # make predictions # predictions &lt;- predict(fit, iris[,1:4]) # # summarize accuracy # table(predictions, iris$Species) # Bagging CART # load the package library(ipred) # load data data(iris) # fit model fit &lt;- bagging(Species~., data=iris) # summarize the fit print(fit) ## ## Bagging classification trees with 25 bootstrap replications ## ## Call: bagging.data.frame(formula = Species ~ ., data = iris) # make predictions predictions &lt;- predict(fit, iris[,1:4], type=&quot;class&quot;) # summarize accuracy table(predictions, iris$Species) ## ## predictions setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 50 1 ## virginica 0 0 49 "],
["improve-accuracy.html", "Chapter 5 06 Improve Accuracy 5.1 Metrics 5.2 Resampling Methods 5.3 Model Selection", " Chapter 5 06 Improve Accuracy 5.1 Metrics # ROC: AUC, sensitivity, specificity metrics # load libraries library(datasets) library(caret) library(mlbench) # load the dataset data(PimaIndiansDiabetes) # prepare resampling method control &lt;- trainControl(method=&quot;cv&quot;, number=5, classProbs=TRUE, summaryFunction=twoClassSummary) set.seed(7) fit &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;glm&quot;, metric=&quot;ROC&quot;, trControl=control) # display results print(fit) ## Generalized Linear Model ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results: ## ## ROC Sens Spec ## 0.8336003 0.882 0.5600978 # RMSE metric # load libraries library(caret) # load data data(longley) # prepare resampling method control &lt;- trainControl(method=&quot;cv&quot;, number=5) set.seed(7) fit &lt;- train(Employed~., data=longley, method=&quot;lm&quot;, metric=&quot;RMSE&quot;, trControl=control) # display results print(fit) ## Linear Regression ## ## 16 samples ## 6 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 12, 12, 14, 13, 13 ## Resampling results: ## ## RMSE Rsquared ## 0.3868618 0.9883114 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE # Accuracy metric # load libraries library(caret) library(mlbench) # load the dataset data(PimaIndiansDiabetes) # prepare resampling method control &lt;- trainControl(method=&quot;cv&quot;, number=5) set.seed(7) fit &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;glm&quot;, metric=&quot;Accuracy&quot;, trControl=control) # display results print(fit) ## Generalized Linear Model ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results: ## ## Accuracy Kappa ## 0.7695442 0.4656824 # Kappa metric # load libraries library(caret) library(mlbench) # load the dataset data(PimaIndiansDiabetes) # prepare resampling method control &lt;- trainControl(method=&quot;cv&quot;, number=5) set.seed(7) fit &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;glm&quot;, metric=&quot;Kappa&quot;, trControl=control) # display results print(fit) ## Generalized Linear Model ## ## 768 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 614, 614, 615, 615, 614 ## Resampling results: ## ## Accuracy Kappa ## 0.7695442 0.4656824 # Rsquared metric # load libraries library(caret) # load data data(longley) # prepare resampling method control &lt;- trainControl(method=&quot;cv&quot;, number=5) set.seed(7) fit &lt;- train(Employed~., data=longley, method=&quot;lm&quot;, metric=&quot;Rsquared&quot;, trControl=control) # display results print(fit) ## Linear Regression ## ## 16 samples ## 6 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 12, 12, 14, 13, 13 ## Resampling results: ## ## RMSE Rsquared ## 0.3868618 0.9883114 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE # MultiNomialLogLoss Metric # load libraries library(caret) # load the dataset data(iris) # prepare resampling method control &lt;- trainControl(method=&quot;cv&quot;, number=5, classProbs=TRUE, summaryFunction=mnLogLoss) set.seed(7) fit &lt;- train(Species~., data=iris, method=&quot;rpart&quot;, metric=&quot;logLoss&quot;, trControl=control) # display results print(fit) ## CART ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 120, 120, 120, 120, 120 ## Resampling results across tuning parameters: ## ## cp logLoss ## 0.00 0.4105613 ## 0.44 0.6840517 ## 0.50 1.0986123 ## ## logLoss was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0. 5.2 Resampling Methods # Estimate accuracy using a train/test split. # load the libraries library(caret) library(klaR) # load the iris dataset data(iris) # define an 80%/20% train/test split of the dataset split=0.80 trainIndex &lt;- createDataPartition(iris$Species, p=split, list=FALSE) data_train &lt;- iris[ trainIndex,] data_test &lt;- iris[-trainIndex,] # train a naive bayes model model &lt;- NaiveBayes(Species~., data=data_train) # make predictions x_test &lt;- data_test[,1:4] y_test &lt;- data_test[,5] predictions &lt;- predict(model, x_test) # summarize results confusionMatrix(predictions$class, y_test) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 10 0 ## virginica 0 0 10 ## ## Overall Statistics ## ## Accuracy : 1 ## 95% CI : (0.8843, 1) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : 4.857e-15 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 1.0000 ## Specificity 1.0000 1.0000 1.0000 ## Pos Pred Value 1.0000 1.0000 1.0000 ## Neg Pred Value 1.0000 1.0000 1.0000 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3333 0.3333 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 1.0000 1.0000 1.0000 # Estimate accuracy using a k-fold cross validation. # load the library library(caret) # load the iris dataset data(iris) # define training control train_control &lt;- trainControl(method=&quot;cv&quot;, number=10) # fix the parameters of the algorithm grid &lt;- expand.grid(fL=c(0), usekernel=c(FALSE)) # train the model model &lt;- train(Species~., data=iris, trControl=train_control, method=&quot;nb&quot;)#tunegrid = grid # summarize results print(model) ## Naive Bayes ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... ## Resampling results across tuning parameters: ## ## usekernel Accuracy Kappa ## FALSE 0.9533333 0.93 ## TRUE 0.9600000 0.94 ## ## Tuning parameter &#39;fL&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;adjust&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were fL = 0, usekernel = TRUE ## and adjust = 1. # Estimate accuracy using repeated k-fold cross validation. # load the library library(caret) # load the iris dataset data(iris) # define training control train_control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # train the model model &lt;- train(Species~., data=iris, trControl=train_control, method=&quot;nb&quot;) # summarize results print(model) ## Naive Bayes ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... ## Resampling results across tuning parameters: ## ## usekernel Accuracy Kappa ## FALSE 0.9466667 0.9200000 ## TRUE 0.9555556 0.9333333 ## ## Tuning parameter &#39;fL&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;adjust&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were fL = 0, usekernel = TRUE ## and adjust = 1. # Estimate accuracy using the bootstrap. # load the library library(caret) # load the iris dataset data(iris) # define training control train_control &lt;- trainControl(method=&quot;boot&quot;, number=100) # train the model model &lt;- train(Species~., data=iris, trControl=train_control, method=&quot;nb&quot;) # summarize results print(model) ## Naive Bayes ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Bootstrapped (100 reps) ## Summary of sample sizes: 150, 150, 150, 150, 150, 150, ... ## Resampling results across tuning parameters: ## ## usekernel Accuracy Kappa ## FALSE 0.9523397 0.9277022 ## TRUE 0.9537120 0.9297424 ## ## Tuning parameter &#39;fL&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;adjust&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were fL = 0, usekernel = TRUE ## and adjust = 1. # Estimate accuracy using a leave one out cross validation. # load the library library(caret) # load the iris dataset data(iris) # define training control train_control &lt;- trainControl(method=&quot;LOOCV&quot;) # train the model model &lt;- train(Species~., data=iris, trControl=train_control, method=&quot;nb&quot;) # summarize results print(model) ## Naive Bayes ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 149, 149, 149, 149, 149, 149, ... ## Resampling results across tuning parameters: ## ## usekernel Accuracy Kappa ## FALSE 0.9533333 0.93 ## TRUE 0.9600000 0.94 ## ## Tuning parameter &#39;fL&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;adjust&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were fL = 0, usekernel = TRUE ## and adjust = 1. 5.3 Model Selection # Compare models using box and whisker plots # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, trControl=control) # LDA set.seed(7) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, trControl=control) # SVM set.seed(7) fit.svm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, trControl=control) # kNN set.seed(7) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, trControl=control) # Random Forest set.seed(7) fit.rf &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rf&quot;, trControl=control) # collect resamples results &lt;- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf)) # box and whisker plots to compare models scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) bwplot(results, scales=scales) # Compare models using density plots plots # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, trControl=control) # LDA set.seed(7) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, trControl=control) # SVM set.seed(7) fit.svm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, trControl=control) # kNN set.seed(7) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, trControl=control) # Random Forest set.seed(7) fit.rf &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rf&quot;, trControl=control) # collect resamples results &lt;- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf)) # density plots of accuracy scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) densityplot(results, scales=scales, pch = &quot;|&quot;) # Compare models using dotplots # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, trControl=control) # LDA set.seed(7) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, trControl=control) # SVM set.seed(7) fit.svm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, trControl=control) # kNN set.seed(7) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, trControl=control) # Random Forest set.seed(7) fit.rf &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rf&quot;, trControl=control) # collect resamples results &lt;- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf)) # dot plots of accuracy scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) dotplot(results, scales=scales) # Compare models using parallel plots # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, trControl=control) # LDA set.seed(7) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, trControl=control) # SVM set.seed(7) fit.svm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, trControl=control) # kNN set.seed(7) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, trControl=control) # Random Forest set.seed(7) fit.rf &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rf&quot;, trControl=control) # collect resamples results &lt;- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf)) # parallel plots to compare models parallelplot(results) # Compare models using scatterplot matrix # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, trControl=control) # LDA set.seed(7) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, trControl=control) # SVM set.seed(7) fit.svm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, trControl=control) # kNN set.seed(7) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, trControl=control) # Random Forest set.seed(7) fit.rf &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rf&quot;, trControl=control) # collect resamples results &lt;- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf)) # pair-wise scatterplots of predictions to compare models splom(results) # pair-wise scatterplots of accuracy measures to compare models splom(results, variables=&quot;metrics&quot;) # Compare models using a table summary # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, trControl=control) # LDA set.seed(7) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, trControl=control) # SVM set.seed(7) fit.svm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, trControl=control) # kNN set.seed(7) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, trControl=control) # Random Forest set.seed(7) fit.rf &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rf&quot;, trControl=control) # collect resamples results &lt;- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf)) # summarize differences between modes summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: CART, LDA, SVM, KNN, RF ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## CART 0.6233766 0.7114662 0.7402597 0.7381864 0.7759740 0.8441558 0 ## LDA 0.6710526 0.7532468 0.7662338 0.7759455 0.8051948 0.8701299 0 ## SVM 0.6710526 0.7402597 0.7582023 0.7650946 0.7889610 0.8961039 0 ## KNN 0.6184211 0.6983510 0.7320574 0.7299385 0.7532468 0.8181818 0 ## RF 0.6842105 0.7296651 0.7582023 0.7625370 0.7922078 0.8571429 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## CART 0.1584699 0.3295590 0.3765182 0.3934073 0.4684788 0.6393443 0 ## LDA 0.2484177 0.4195842 0.4515939 0.4800991 0.5511677 0.7047546 0 ## SVM 0.2187500 0.3888801 0.4167479 0.4520197 0.5002572 0.7638037 0 ## KNN 0.1112903 0.3228267 0.3866876 0.3818558 0.4382002 0.5866564 0 ## RF 0.2852665 0.3860329 0.4552584 0.4613026 0.5168514 0.6780692 0 # Compare models using xyplot # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, trControl=control) # LDA set.seed(7) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, trControl=control) # SVM set.seed(7) fit.svm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, trControl=control) # kNN set.seed(7) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, trControl=control) # Random Forest set.seed(7) fit.rf &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rf&quot;, trControl=control) # collect resamples results &lt;- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf)) # xyplot plots to compare models xyplot(results, models=c(&quot;LDA&quot;, &quot;SVM&quot;)) # Calculate statistical significance of difference between model predictions # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, trControl=control) # LDA set.seed(7) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, trControl=control) # SVM set.seed(7) fit.svm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, trControl=control) # kNN set.seed(7) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, trControl=control) # Random Forest set.seed(7) fit.rf &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rf&quot;, trControl=control) # collect resamples results &lt;- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf)) # difference in model predictions diffs &lt;- diff(results) # summarize p-values for pair-wise comparisons summary(diffs) ## ## Call: ## summary.diff.resamples(object = diffs) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## Accuracy ## CART LDA SVM KNN RF ## CART -0.037759 -0.026908 0.008248 -0.024351 ## LDA 0.0050068 0.010851 0.046007 0.013409 ## SVM 0.0919580 0.3390336 0.035156 0.002558 ## KNN 1.0000000 1.218e-05 0.0007092 -0.032599 ## RF 0.0974572 0.2416366 1.0000000 0.0016066 ## ## Kappa ## CART LDA SVM KNN RF ## CART -0.086692 -0.058612 0.011552 -0.067895 ## LDA 0.0015478 0.028079 0.098243 0.018796 ## SVM 0.0839950 0.2217402 0.070164 -0.009283 ## KNN 1.0000000 4.122e-05 0.0054686 -0.079447 ## RF 0.0129477 1.0000000 1.0000000 0.0006918 # plot of differences scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) bwplot(diffs, scales=scales) # t-test between two models compare_models(fit.svm, fit.lda) ## ## One Sample t-test ## ## data: x ## t = -2.2266, df = 29, p-value = 0.0339 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.0208181758 -0.0008838064 ## sample estimates: ## mean of x ## -0.01085099 "],
["improve-accuracy-1.html", "Chapter 6 Improve Accuracy 6.1 Tuning 6.2 Ensembles", " Chapter 6 Improve Accuracy 6.1 Tuning # Randomly search algorithm parameters # load the library library(caret) # load the dataset data(iris) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3, search=&quot;random&quot;) # train the model model &lt;- train(Species~., data=iris, method=&quot;lvq&quot;, trControl=control, tuneLength=25) # summarize the model print(model) ## Learning Vector Quantization ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... ## Resampling results across tuning parameters: ## ## size k Accuracy Kappa ## 5 15 0.9511111 0.92666667 ## 5 37 0.9333333 0.90000000 ## 5 57 0.9355556 0.90333333 ## 5 70 0.9488889 0.92333333 ## 5 96 0.6666667 0.50000000 ## 5 98 0.6666667 0.50000000 ## 6 5 0.9444444 0.91666667 ## 6 34 0.9422222 0.91333333 ## 6 38 0.9466667 0.92000000 ## 6 87 0.9400000 0.91000000 ## 6 136 0.3333333 0.00000000 ## 7 14 0.9511111 0.92666667 ## 7 24 0.9511111 0.92666667 ## 7 29 0.9466667 0.92000000 ## 7 52 0.9466667 0.92000000 ## 7 72 0.9422222 0.91333333 ## 8 74 0.9466667 0.92000000 ## 8 92 0.6666667 0.50000000 ## 8 128 0.3333333 0.00000000 ## 9 62 0.9488889 0.92333333 ## 10 8 0.9622222 0.94333333 ## 10 102 0.3444444 0.01666667 ## 10 105 0.3333333 0.00000000 ## 10 107 0.3333333 0.00000000 ## 10 125 0.3333333 0.00000000 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were size = 10 and k = 8. # plot the effect of parameters on accuracy plot(model) # Tune algorithm parameters using an automatic grid search. # load the library library(caret) # load the dataset data(iris) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # train the model model &lt;- train(Species~., data=iris, method=&quot;lvq&quot;, trControl=control, tuneLength=5) # summarize the model print(model) ## Learning Vector Quantization ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... ## Resampling results across tuning parameters: ## ## size k Accuracy Kappa ## 5 1 0.9577778 0.9366667 ## 5 6 0.9488889 0.9233333 ## 5 11 0.9577778 0.9366667 ## 5 16 0.9355556 0.9033333 ## 5 21 0.9333333 0.9000000 ## 6 1 0.9444444 0.9166667 ## 6 6 0.9400000 0.9100000 ## 6 11 0.9355556 0.9033333 ## 6 16 0.9333333 0.9000000 ## 6 21 0.9355556 0.9033333 ## 7 1 0.9333333 0.9000000 ## 7 6 0.9444444 0.9166667 ## 7 11 0.9355556 0.9033333 ## 7 16 0.9400000 0.9100000 ## 7 21 0.9533333 0.9300000 ## 8 1 0.9644444 0.9466667 ## 8 6 0.9644444 0.9466667 ## 8 11 0.9644444 0.9466667 ## 8 16 0.9600000 0.9400000 ## 8 21 0.9466667 0.9200000 ## 10 1 0.9666667 0.9500000 ## 10 6 0.9511111 0.9266667 ## 10 11 0.9666667 0.9500000 ## 10 16 0.9511111 0.9266667 ## 10 21 0.9511111 0.9266667 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were size = 10 and k = 11. # plot the effect of parameters on accuracy plot(model) # Select the best tuning configuration # load libraries library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) tunegrid &lt;- expand.grid(.cp=seq(0,0.1,by=0.01)) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rpart&quot;, metric=&quot;Accuracy&quot;, tuneGrid=tunegrid, trControl=control) # display the best configuration print(fit.cart$bestTune) ## cp ## 5 0.04 # Customer Parameter Search # load the packages library(randomForest) library(mlbench) library(caret) # configure multi-core (not supported on Windoews) library(doMC) registerDoMC(cores=8) # define the custom caret algorithm (wrapper for Random Forest) customRF &lt;- list(type=&quot;Classification&quot;, library=&quot;randomForest&quot;, loop=NULL) customRF$parameters &lt;- data.frame(parameter=c(&quot;mtry&quot;, &quot;ntree&quot;), class=rep(&quot;numeric&quot;, 2), label=c(&quot;mtry&quot;, &quot;ntree&quot;)) customRF$grid &lt;- function(x, y, len=NULL, search=&quot;grid&quot;) {} customRF$fit &lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) { randomForest(x, y, mtry=param$mtry, ntree=param$ntree, ...) } customRF$predict &lt;- function(modelFit, newdata, preProc=NULL, submodels=NULL) predict(modelFit, newdata) customRF$prob &lt;- function(modelFit, newdata, preProc=NULL, submodels=NULL) predict(modelFit, newdata, type = &quot;prob&quot;) customRF$sort &lt;- function(x) x[order(x[,1]),] customRF$levels &lt;- function(x) x$classes # Load Dataset data(Sonar) dataset &lt;- Sonar seed &lt;- 7 metric &lt;- &quot;Accuracy&quot; # train model trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) tunegrid &lt;- expand.grid(.mtry=c(1:15), .ntree=c(1000, 1500, 2000, 2500)) set.seed(seed) custom &lt;- train(Class~., data=dataset, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=trainControl) print(custom) ## 208 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 187, 188, 188, 187, 187, 187, ... ## Resampling results across tuning parameters: ## ## mtry ntree Accuracy Kappa ## 1 1000 0.8378860 0.6693068 ## 1 1500 0.8314574 0.6567188 ## 1 2000 0.8394012 0.6723757 ## 1 2500 0.8345599 0.6624305 ## 2 1000 0.8378139 0.6698793 ## 2 1500 0.8459091 0.6854461 ## 2 2000 0.8427345 0.6792114 ## 2 2500 0.8492352 0.6927712 ## 3 1000 0.8395527 0.6736726 ## 3 1500 0.8334343 0.6604164 ## 3 2000 0.8362266 0.6661627 ## 3 2500 0.8298773 0.6533552 ## 4 1000 0.8254185 0.6443563 ## 4 1500 0.8267027 0.6475804 ## 4 2000 0.8331962 0.6601512 ## 4 2500 0.8331241 0.6601224 ## 5 1000 0.8282107 0.6507048 ## 5 1500 0.8235931 0.6406093 ## 5 2000 0.8235209 0.6408543 ## 5 2500 0.8155844 0.6240125 ## 6 1000 0.8218543 0.6377897 ## 6 1500 0.8153463 0.6236070 ## 6 2000 0.8236003 0.6404547 ## 6 2500 0.8219336 0.6375842 ## 7 1000 0.8186003 0.6305917 ## 7 1500 0.8233622 0.6396018 ## 7 2000 0.8153463 0.6234899 ## 7 2500 0.8281241 0.6491065 ## 8 1000 0.8204113 0.6340540 ## 8 1500 0.8250289 0.6431140 ## 8 2000 0.8250361 0.6436154 ## 8 2500 0.8203463 0.6339480 ## 9 1000 0.8283622 0.6503540 ## 9 1500 0.8252597 0.6445064 ## 9 2000 0.8170996 0.6274325 ## 9 2500 0.8203391 0.6334506 ## 10 1000 0.8233622 0.6398523 ## 10 1500 0.8202670 0.6337966 ## 10 2000 0.8170202 0.6274176 ## 10 2500 0.8218543 0.6366811 ## 11 1000 0.8203463 0.6340067 ## 11 1500 0.8170058 0.6268416 ## 11 2000 0.8122439 0.6171392 ## 11 2500 0.8187518 0.6306184 ## 12 1000 0.8186724 0.6306219 ## 12 1500 0.8121645 0.6174541 ## 12 2000 0.8157287 0.6240841 ## 12 2500 0.8138312 0.6202156 ## 13 1000 0.8157359 0.6247596 ## 13 1500 0.8123954 0.6178441 ## 13 2000 0.8220058 0.6373383 ## 13 2500 0.8156494 0.6239558 ## 14 1000 0.8121717 0.6169495 ## 14 1500 0.8059740 0.6039901 ## 14 2000 0.8154978 0.6242557 ## 14 2500 0.8171717 0.6281273 ## 15 1000 0.8108947 0.6150481 ## 15 1500 0.8138312 0.6202945 ## 15 2000 0.8125613 0.6181653 ## 15 2500 0.8155772 0.6239136 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 2 and ntree = 2500. plot(custom) # Tune algorithm parameters using a manual grid search. # load the library library(caret) # load the dataset data(iris) # prepare training scheme control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) # design the parameter tuning grid grid &lt;- expand.grid(size=c(5,10,20,50), k=c(1,2,3,4,5)) # train the model model &lt;- train(Species~., data=iris, method=&quot;lvq&quot;, trControl=control, tuneGrid=grid) # summarize the model print(model) ## Learning Vector Quantization ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... ## Resampling results across tuning parameters: ## ## size k Accuracy Kappa ## 5 1 0.9444444 0.9166667 ## 5 2 0.9511111 0.9266667 ## 5 3 0.9355556 0.9033333 ## 5 4 0.9400000 0.9100000 ## 5 5 0.9600000 0.9400000 ## 10 1 0.9688889 0.9533333 ## 10 2 0.9622222 0.9433333 ## 10 3 0.9511111 0.9266667 ## 10 4 0.9666667 0.9500000 ## 10 5 0.9555556 0.9333333 ## 20 1 0.9644444 0.9466667 ## 20 2 0.9622222 0.9433333 ## 20 3 0.9711111 0.9566667 ## 20 4 0.9600000 0.9400000 ## 20 5 0.9622222 0.9433333 ## 50 1 0.9666667 0.9500000 ## 50 2 0.9644444 0.9466667 ## 50 3 0.9600000 0.9400000 ## 50 4 0.9688889 0.9533333 ## 50 5 0.9600000 0.9400000 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were size = 20 and k = 3. # plot the effect of parameters on accuracy plot(model) # Manually search parametres # load the packages library(randomForest) library(mlbench) library(caret) # Load Dataset data(Sonar) dataset &lt;- Sonar x &lt;- dataset[,1:60] y &lt;- dataset[,61] seed &lt;- 7 metric &lt;- &quot;Accuracy&quot; # Manual Search trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3, search=&quot;grid&quot;) tunegrid &lt;- expand.grid(.mtry=c(sqrt(ncol(x)))) modellist &lt;- list() for (ntree in c(1000, 1500, 2000, 2500)) { set.seed(seed) fit &lt;- train(Class~., data=dataset, method=&quot;rf&quot;, metric=metric, tuneGrid=tunegrid, trControl=trainControl, ntree=ntree) key &lt;- toString(ntree) modellist[[key]] &lt;- fit } # compare results results &lt;- resamples(modellist) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: 1000, 1500, 2000, 2500 ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1000 0.6190476 0.8095238 0.8340909 0.8267821 0.8892857 0.9090909 0 ## 1500 0.6000000 0.8000000 0.8095238 0.8202670 0.8892857 0.9090909 0 ## 2000 0.6000000 0.8000000 0.8095238 0.8234416 0.9000000 0.9090909 0 ## 2500 0.6000000 0.8000000 0.8095238 0.8186003 0.8571429 0.9090909 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1000 0.2222222 0.6111111 0.6573093 0.6475562 0.7740939 0.8166667 0 ## 1500 0.1397849 0.5876289 0.6164304 0.6335420 0.7740939 0.8166667 0 ## 2000 0.1397849 0.5811177 0.6164304 0.6403175 0.7979798 0.8166667 0 ## 2500 0.1397849 0.5811177 0.6164304 0.6303666 0.7149321 0.8166667 0 dotplot(results) 6.2 Ensembles # Stacking (non-linear combination of models) # load libraries library(caret) library(caretEnsemble) # load the dataset data(PimaIndiansDiabetes) # define training control train_control &lt;- trainControl(method=&quot;cv&quot;, number=10, savePredictions=TRUE, classProbs=TRUE) # train a list of models methodList &lt;- c(&#39;glm&#39;, &#39;lda&#39;, &#39;knn&#39;) models &lt;- caretList(diabetes~., data=PimaIndiansDiabetes, trControl=train_control, methodList=methodList) # create stacked ensemble of trained models ensemble &lt;- caretStack(models, method=&#39;rpart&#39;) # summarize ensemble summary(ensemble) ## Call: ## rpart(formula = .outcome ~ ., data = list(glm = c(0.953051257162688, ## 0.958095739101053, 0.85887916688888, 0.314809950674701, 0.752185013551774, ## 0.184555913091376, 0.650108014935502, 0.600063606290071, 0.696026046535039, ## 0.958945354456846, 0.483256811189423, 0.722874779481644, 0.524244287892272, ## 0.952401953246284, 0.961070988645185, 0.528629535656046, 0.848212261104731, ## 0.325720565159536, 0.265062837824198, 0.306239835944601, 0.880975557961356, ## 0.346692761596444, 0.584431374374848, 0.954954036992555, 0.969715130944145, ## 0.963707429906792, 0.910720506246518, 0.930524215585373, 0.299571954911195, ## 0.977424207918859, 0.642343185593917, 0.132572718831246, 0.833834420262351, ## 0.989901285751701, 0.972594088337254, 0.686323796835962, 0.884257063028402, ## 0.481236542895937, 0.9677376303654, 0.700920611251387, 0.637718071929156, ## 0.755256816269779, 0.951657580936434, 0.998341075297739, 0.91125379831104, ## 0.784124489003413, 0.894658522105206, 0.901288067990899, 0.995022496308702, ## 0.865912094060224, 0.948109892432111, 0.806021663976616, 0.419478356306152, ## 0.838472435046698, 0.937304979998552, 0.980415137633297, 0.736478867836674, ## 0.63880652466032, 0.751534422543387, 0.494652222126384, 0.917765331153936, ## 0.98453622186921, 0.861841179306258, 0.718882858123029, 0.92852820049092, ## 0.96907068421229, 0.791185614974929, 0.760351456197514, 0.975557558262242, ## 0.555995892087884, 0.876036546040046, 0.94589374195788, 0.907704428010341, ## 0.841712033178728, 0.872289721293513, 0.940102606300061, 0.72895856467944, ## 0.853996482793259, 0.683397153987871, 0.547731565660033, 0.784110136206764, ## 0.738212156579165, 0.942750326070168, 0.762020114507566, 0.893600734794305, ## 0.943714418227953, 0.742130176811609, 0.821348230278369, 0.854817917011086, ## 0.674573981560566, 0.812317726287503, 0.50265225838879, 0.989409350613652, ## 0.917253692862616, 0.688278097216888, 0.386725256226286, 0.949382343025537, ## 0.648429594517848, 0.874958115454339, 0.442016623644194, 0.909844416172741, ## 0.871810988139328, 0.945194901499105, 0.534232238112662, 0.700121832034019, ## 0.778351318633411, 0.898461705835151, 0.553741772876854, 0.667423535664109, ## 0.759015998604891, 0.87105580116034, 0.843928322611711, 0.807229639916422, ## 0.946674458566967, 0.87350489990668, 0.230246018898447, 0.952860389996093, ## 0.735259964101078, 0.998012539874804, 0.9498054841721, 0.665286088061955, ## 0.925201268178742, 0.546740461624441, 0.898955695596301, 0.945714249379185, ## 0.830012992540059, 0.548381864007711, 0.874172658679228, 0.958706705438238, ## 0.689673371283455, 0.877899488513011, 0.905357962341416, 0.945738328826993, ## 0.40565868120276, 0.153708371384636, 0.66572631665473, 0.641404787945913, ## 0.344866401626344, 0.934817084210213, 0.930497380566112, 0.887900962395227, ## 0.0254366695970163, 0.701383904483636, 0.969001144199781, 0.661712430919261, ## 0.949157998879512, 0.954795818023797, 0.943218065659777, 0.870023030891881, ## 0.529466593997153, 0.521450100859371, 0.24048749144298, 0.589078282916621, ## 0.90456383829272, 0.6385725886185, 0.824909132978535, 0.973442417569207, ## 0.918590313847628, 0.759014147953176, 0.890642216501112, 0.314287983904611, ## 0.190656023048842, 0.844185650014582, 0.511150029996566, 0.70578108302244, ## 0.353757122570861, 0.948746222462965, 0.915813000758116, 0.870319403553065, ## 0.956639101880133, 0.475412882566585, 0.709644779826367, 0.91967319019534, ## 0.739279219251595, 0.874098574509036, 0.401970661625533, 0.595173894771048, ## 0.540015447082072, 0.386922208931911, 0.941691217411074, 0.746524603260119, ## 0.939497562550129, 0.575218706704547, 0.257089297572756, 0.858070965108653, ## 0.650955374033442, 0.91054434401173, 0.771832822924239, 0.715359460205826, ## 0.880774626029125, 0.937518488722379, 0.813882372097382, 0.783042125538073, ## 0.811762289589769, 0.960936440984069, 0.717715401294037, 0.719389690577033, ## 0.82745153519684, 0.766641780841237, 0.0924769679688028, 0.842978803965421, ## 0.486442513897395, 0.923067304811416, 0.812685481417125, 0.954888234773378, ## 0.270016558229892, 0.400758215593288, 0.834577114979937, 0.936697351021979, ## 0.99601445577501, 0.681571776268354, 0.581963494856109, 0.432513105841615, ## 0.640257752520974, 0.786611439172398, 0.949786817291227, 0.799060035371083, ## 0.671945612679124, 0.939398224164642, 0.932073643658478, 0.776042983316105, ## 0.670507398133359, 0.299626049638312, 0.658008898433258, 0.547943784337918, ## 0.75362588449137, 0.965857524513019, 0.967010382243772, 0.959433841181008, ## 0.912462859716047, 0.854786835581599, 0.553595583257467, 0.970788630781364, ## 0.873885133097866, 0.784150263798224, 0.84047550426649, 0.955594366869655, ## 0.887586212744108, 0.892014692740623, 0.903069755770341, 0.909705329322654, ## 0.808976278981038, 0.866226329211781, 0.892336099257971, 0.876795863846395, ## 0.579887017094107, 0.87813605998835, 0.968114508178456, 0.632775042370563, ## 0.89521959237679, 0.488315217879056, 0.954170266074032, 0.721061747072713, ## 0.822624964817125, 0.342855737123821, 0.788420866061083, 0.883078006508762, ## 0.976234020602672, 0.613475857668185, 0.935193773009691, 0.76548808938973, ## 0.813428625100329, 0.989169757545747, 0.69511344000926, 0.895345983689389, ## 0.914506139571774, 0.964179488490031, 0.808675535379847, 0.89524921482404, ## 0.235297598376937, 0.50066245459128, 0.977127059482183, 0.660614181903302, ## 0.920979129271868, 0.781192098861976, 0.912123695292493, 0.904609106425321, ## 0.862231183512567, 0.975018019491638, 0.887727534041828, 0.626005474057837, ## 0.761062233104523, 0.58989896061867, 0.910188762679297, 0.352927244645287, ## 0.68889122233863, 0.977885683618116, 0.807967226240266, 0.926192719904255, ## 0.677231829889067, 0.885893833461238, 0.976480825271385, 0.867663549810815, ## 0.140907095146263, 0.421565130353645, 0.693280760738865, 0.708207214935352, ## 0.562035541097763, 0.8603835138909, 0.776139047936548, 0.827648854515767, ## 0.695720440681508, 0.657940232750353, 0.832866641645026, 0.918099217652215, ## 0.934291099600117, 0.569853208220505, 0.0691540279418732, 0.931109662674831, ## 0.0765633303985364, 0.85673425448684, 0.907182545053234, 0.828787039125085, ## 0.987561465386588, 0.29690699594852, 0.844426100390946, 0.939775730533387, ## 0.371989566943227, 0.924683539026673, 0.883983670107772, 0.708880961773285, ## 0.848113484786559, 0.867751813541948, 0.770087391944057, 0.872787704537997, ## 0.688574790098755, 0.886120753764587, 0.839384445809357, 0.928108313406017, ## 0.931671239448513, 0.457435756325279, 0.783295425764292, 0.80623275234043, ## 0.980277981306003, 0.754123688824696, 0.958102303986293, 0.750939335346643, ## 0.954071899988049, 0.972226351364737, 0.902160971627735, 0.869576085881861, ## 0.877530319311754, 0.756081073636111, 0.651741090797322, 0.774241186878109, ## 0.740080708407008, 0.858384180777407, 0.903354650719075, 0.966715625804739, ## 0.70451839187884, 0.900764749192881, 0.93357617264099, 0.71800315706017, ## 0.421521140603624, 0.239092699245138, 0.889203851501047, 0.899923403603129, ## 0.773988623207612, 0.931030760047848, 0.892776050317642, 0.797752249921897, ## 0.843218477096792, 0.716993976663318, 0.318785505081014, 0.803567118914535, ## 0.885934767330098, 0.827659809518092, 0.911115825376754, 0.893406268445303, ## 0.821177198542335, 0.852747201689708, 0.481542877196625, 0.923684808792929, ## 0.91248848773185, 0.818192058831394, 0.872933674746896, 0.695806912801524, ## 0.720217141473725, 0.731928749973023, 0.527280052497651, 0.857261298544614, ## 0.544304793110193, 0.67036318034099, 0.944406394019484, 0.90459041968017, ## 0.964141852928657, 0.815431602746048, 0.801537284127708, 0.589122200296171, ## 0.879783717696365, 0.894410380971727, 0.903475488408534, 0.917301302254272, ## 0.81874041554115, 0.861218482661572, 0.636474925202552, 0.965395555585249, ## 0.534676289268003, 0.946777734041253, 0.859965474993015, 0.751109196329137, ## 0.911031518462322, 0.843472685819034, 0.985624501083974, 0.812187003629612, ## 0.830064012654038, 0.0287430028663036, 0.859760505298375, 0.895815904707375, ## 0.850735140678052, 0.893808478903923, 0.759505502198521, 0.589361129355288, ## 0.935168204556172, 0.895347148545943, 0.891750340901, 0.890989931205683, ## 0.839603705377719, 0.858847682144187, 0.914953297994293, 0.972202020259574, ## 0.907092728332423, 0.652760293887727, 0.754646373078921, 0.86850736701707, ## 0.482796588551785, 0.92432674414464, 0.952286455305987, 0.765154088560804, ## 0.651700474233453, 0.805285648433757, 0.88806076000289, 0.956779540044917, ## 0.514882097640229, 0.381795418857065, 0.282921174490116, 0.852875424780943, ## 0.752129588745917, 0.364616084235841, 0.215907970128839, 0.909331341417803, ## 0.920436740033271, 0.28412450471618, 0.625970904316298, 0.912706543145811, ## 0.92839211172917, 0.983652942674685, 0.798305272577695, 0.900646942322836, ## 0.716085225144712, 0.834798461705615, 0.882156791371878, 0.793551363310189, ## 0.764481558218139, 0.55561597419498, 0.966303900920227, 0.880357437770405, ## 0.681453177862017, 0.364261470793222, 0.748251138125274, 0.292141312552872, ## 0.897358776974594, 0.838997088185604, 0.753674095535677, 0.565230565218162, ## 0.665922470898218, 0.866156433000942, 0.896411566279414, 0.807862042760691, ## 0.812371818617291, 0.927976252623603, 0.800979624283995, 0.657832859336935, ## 0.837363325899522, 0.662137267944593, 0.793759270551226, 0.71689561237428, ## 0.588434030160761, 0.916129407744689, 0.883672392843513, 0.897696310956275, ## 0.844601417575414, 0.891480116392841, 0.884679909530382, 0.833925136947224, ## 0.823467824338458, 0.906043747524223, 0.0526627904174204, 0.708473457277871, ## 0.669295665689467, 0.736061886079314, 0.881779386351392, 0.543211488043799, ## 0.845311200006498, 0.910035324959093, 0.90833730745375, 0.650611494779749, ## 0.679431631865378, 0.835674534801532, 0.928539657162173, 0.272106648748904, ## 0.231666765503033, 0.130057549879654, 0.942609150843695, 0.317122821984511, ## 0.9726033410941, 0.0927252172766138, 0.459991134642979, 0.406248794735627, ## 0.642943328649834, 0.611529177414958, 0.809625048141723, 0.76289519250281, ## 0.0613089632455827, 0.700175598485297, 0.342564800190591, 0.534199005541661, ## 0.279936528516168, 0.4314029614188, 0.601458160637947, 0.832272743569696, ## 0.452397373033104, 0.0676040260726758, 0.0424756757249496, 0.640452276727185, ## 0.167993563600905, 0.12516180419896, 0.496187097132029, 0.65589491351399, ## 0.83449835088116, 0.805559248828191, 0.151566028736848, 0.338178190710291, ## 0.353662012823034, 0.255756503806678, 0.719185775467216, 0.568214088596754, ## 0.149919286013173, 0.926738093848699, 0.412666533702721, 0.324885016757837, ## 0.274283716645206, 0.423833751228996, 0.635107772167807, 0.120133583949661, ## 0.882306377708141, 0.526264468477248, 0.816475754827646, 0.881676474016428, ## 0.353189423352059, 0.308212812632115, 0.317543167214789, 0.61323438148995, ## 0.136356925386515, 0.040197363717005, 0.109068487087956, 0.0202293609762977, ## 0.687952879316395, 0.764019804782834, 0.847602923777232, 0.479398081143438, ## 0.120003279679331, 0.173462165156495, 0.356131242504368, 0.0528132581854618, ## 0.163318274904321, 0.637325484433032, 0.783937586279812, 0.651396050713305, ## 0.304838947468606, 0.0248235425833603, 0.30026660728624, 0.8919121028812, ## 0.667524270735696, 0.637592582560341, 0.0516572891198053, 0.345352655253291, ## 0.123941827189109, 0.454039981761043, 0.707855078289124, 0.091307508849368, ## 0.711904643925655, 0.858454192061691, 0.629560302192659, 0.347571487393558, ## 0.342789515787037, 0.225151102974897, 0.312454096103199, 0.35460298602803, ## 0.132863120057671, 0.113310181418943, 0.139974549573371, 0.181727950021769, ## 0.715315967267591, 0.564876994761429, 0.0606108904601323, 0.713633121869115, ## 0.786340727525265, 0.0904317120354392, 0.309274185780802, 0.702752675210759, ## 0.301981843179724, 0.498093977434194, 0.24658510518276, 0.8321741802016, ## 0.50054573410485, 0.381192942699581, 0.896884953112499, 0.550622074007178, ## 0.77002472973474, 0.377583503342026, 0.492337402076268, 0.730393838624738, ## 0.556016123808815, 0.156382083440798, 0.603820761546813, 0.465068714633297, ## 0.328707403433914, 0.612955576994717, 0.680067243472388, 0.648443400678101, ## 0.478375982373306, 0.292114092716277, 0.182731732046635, 0.843476450892441, ## 0.849027144014106, 0.250676374908062, 0.674683887880139, 0.795769962474026, ## 0.0692681793236252, 0.708674559480369, 0.250234518738859, 0.097775864560453, ## 0.985874006493701, 0.258492205136936, 0.625713847629909, 0.0988863836404267, ## 0.131568367629994, 0.152277775021846, 0.250846736235051, 0.740895240653199, ## 0.76879299030626, 0.0858402524271203, 0.189720676540352, 0.15084173548894, ## 0.648880835775545, 0.576627380920177, 0.47707158714565, 0.156814373503291, ## 0.336345600750023, 0.77541935441276, 0.182987644543678, 0.874109070620746, ## 0.591500281092378, 0.267260072769927, 0.761380215467783, 0.0735770728142736, ## 0.236243201219388, 0.643347434738104, 0.510675563641594, 0.362828933981565, ## 0.796966882072902, 0.218196826669409, 0.212944582823979, 0.257359063035486, ## 0.905116429994271, 0.24219592367263, 0.25607990607773, 0.622761136799391, ## 0.776766984847271, 0.00571676927718823, 0.874580583068588, 0.726529910613456, ## 0.0787858343193321, 0.146889404211399, 0.497791496250756, 0.835114916321656, ## 0.396796716486196, 0.137293476117833, 0.590059173721881, 0.542683208826364, ## 0.171249106885273, 0.987189413624017, 0.347245645256558, 0.773219729623166, ## 0.470333531928236, 0.320963985004586, 0.339515721945948, 0.42042635739715, ## 0.569134078161482, 0.595501452823324, 0.709286642515049, 0.620355336945796, ## 0.163374830598383, 0.0379924721596566, 0.571404057259625, 0.112063169301446, ## 0.819933125003774, 0.577444409724361, 0.0629173392474929, 0.483824377106846, ## 0.743874223373336, 0.373832951349461, 0.151549571716357, 0.225111805548601, ## 0.568006598657497, 0.309137367128982, 0.338822657104436, 0.245596744543245, ## 0.17167040623459, 0.16407890973727, 0.201802872582866, 0.173318868766561, ## 0.240078065995761, 0.52941274050642, 0.7048521851553, 0.667735050691016, ## 0.587220003354867, 0.57665600292064, 0.499484384154365, 0.620222257679386, ## 0.2483489177272, 0.535487513916298, 0.507408560926349, 0.879300957175119, ## 0.036381465280329, 0.292768091366636, 0.248798828906604, 0.603572281374729, ## 0.428116453550079, 0.752586810821186, 0.116610065515921, 0.414306333703375, ## 0.69512736542406, 0.195382187736053, 0.694539971586895, 0.331798416114298, ## 0.12592954977003, 0.363438847435047, 0.652005236260279, 0.466422102871516, ## 0.636183125959228, 0.261959181310596, 0.917252667495865, 0.212589314313651, ## 0.845770373843264, 0.196165266136854, 0.0923302715511966, 0.2266156609053, ## 0.737279725837658, 0.624297444183182, 0.774079122796547, 0.801416881354341, ## 0.145164571680543, 0.799146801739213, 0.275559414263591, 0.347821890259785, ## 0.318338995449735, 0.172793401483356, 0.445199733609588, 0.449419321886982, ## 0.369772967563184, 0.303471833456679, 0.564185309608125, 0.736748401883241, ## 0.0877392350430801, 0.0666135055876599, 0.753790262697099), lda = c(0.958386071986982, ## 0.962978116004363, 0.858860217087358, 0.302858474432556, 0.783197688522066, ## 0.174161309812446, 0.668555713681069, 0.624310823371787, 0.710179053477544, ## 0.959462892286783, 0.452503339938082, 0.737301154036704, 0.536196685727766, ## 0.956450608381185, 0.962526820383922, 0.51699956527577, 0.854661171010248, ## 0.324148417172017, 0.246776657542315, 0.325246085640529, 0.884088831020892, ## 0.329109427153581, 0.581168947611315, 0.961764495060892, 0.96062518819204, ## 0.966389451948156, 0.916314044914605, 0.933953692874142, 0.28434195402187, ## 0.980261072279381, 0.69211179799297, 0.143036503565644, 0.856359950401639, ## 0.988751458871996, 0.976632598842594, 0.678963233843763, 0.888475653533751, ## 0.529272498859367, 0.969922917921784, 0.696910412012859, 0.630191761076957, ## 0.76927025670527, 0.960080296474759, 0.998784654886086, 0.922136281944625, ## 0.808982959077383, 0.896358296876791, 0.896842877793057, 0.994496992427683, ## 0.87496444097063, 0.949983672597486, 0.822989684874879, 0.423855389935205, ## 0.85614424949594, 0.940777960546582, 0.982283967295099, 0.741737443743037, ## 0.688127480491527, 0.756344116949857, 0.486312565399349, 0.92712194365985, ## 0.985983196760874, 0.870155483400618, 0.708658935597499, 0.934299984824972, ## 0.973203871741432, 0.821448248979317, 0.765832271025841, 0.979907663624039, ## 0.542699714935338, 0.888079331845588, 0.951450225135068, 0.923027265004947, ## 0.863191393249192, 0.880647256483685, 0.944602041114814, 0.742423823241986, ## 0.869304692723384, 0.669172819778843, 0.578978962085491, 0.793845163107677, ## 0.763728880889641, 0.944356759331879, 0.770083062995283, 0.905691867498117, ## 0.948595486008516, 0.750933376908073, 0.82929549038697, 0.849615612697729, ## 0.701425281914949, 0.822486539156496, 0.489441649974845, 0.989783619902501, ## 0.926217292783958, 0.707218929288963, 0.3795979259534, 0.953255786001237, ## 0.665812175397134, 0.874086464408997, 0.46452917154119, 0.914638506335317, ## 0.87478590418803, 0.951792326007555, 0.535227006470422, 0.718464270695789, ## 0.802735736211407, 0.908313373930357, 0.558060357574317, 0.677235099691955, ## 0.772023079200859, 0.878923343771717, 0.844182275793972, 0.837237408347843, ## 0.954316117237653, 0.883141786888738, 0.238180583667982, 0.955736022963286, ## 0.752940765114063, 0.998674671901048, 0.955780789090023, 0.657180994864932, ## 0.926984870712937, 0.538602505614664, 0.901436513906793, 0.94833197148848, ## 0.839823230875183, 0.57891050420251, 0.884400258338038, 0.9591392939219, ## 0.696643400877908, 0.879094684052161, 0.913613189027244, 0.952027932289082, ## 0.433701169532446, 0.144967788732986, 0.667940474924718, 0.602585884259633, ## 0.326239637103283, 0.935016691857309, 0.93986320693184, 0.90245919925514, ## 0.0221363687682808, 0.735157907522305, 0.973869996564253, 0.679006801945198, ## 0.954543846181255, 0.955915056994611, 0.947572655995322, 0.88349975228237, ## 0.540076251705138, 0.519241662705025, 0.274202451603586, 0.581113529500204, ## 0.913934077763295, 0.636584232175904, 0.831718587675857, 0.975359597204615, ## 0.93042325815893, 0.770748196537027, 0.895911507023222, 0.279225542933773, ## 0.173907072641134, 0.856322079789522, 0.507683871949615, 0.724867987294729, ## 0.371085308624374, 0.950186167379057, 0.9186265123542, 0.871268299302865, ## 0.964308875580655, 0.482774704930944, 0.739569429031387, 0.927428291430677, ## 0.737374445562159, 0.883081935134013, 0.402326347236249, 0.600902221133193, ## 0.525962599455899, 0.390756533068436, 0.940734826940288, 0.758278837765637, ## 0.950382445056503, 0.533817040116715, 0.252376086409707, 0.86260571225304, ## 0.647485115640529, 0.924427675850855, 0.757212818504231, 0.738484959438813, ## 0.878221652963055, 0.941002893899476, 0.834046883700365, 0.79067414469193, ## 0.822844265927114, 0.962968106685266, 0.734633932952272, 0.711082476185447, ## 0.847945915767999, 0.756415969239434, 0.0870247319198728, 0.847699679977736, ## 0.485860656698846, 0.933323411956384, 0.804912875150198, 0.956288716962979, ## 0.285532172117958, 0.406638948248937, 0.83567251379326, 0.940418674468521, ## 0.997348048976993, 0.699281504922466, 0.600106856889928, 0.441797520282961, ## 0.63559225329336, 0.770146895697346, 0.948921207902361, 0.829088230862945, ## 0.678309754284553, 0.947912961363342, 0.940249640980874, 0.811795315336768, ## 0.680672093224644, 0.271706172227794, 0.68344552109255, 0.551841813450512, ## 0.764785572184144, 0.967837889976039, 0.972428041359937, 0.954927421653893, ## 0.92486188119136, 0.865802248139708, 0.566721801477324, 0.972875080403133, ## 0.890496354722351, 0.82031549409925, 0.854910824664846, 0.958961060740696, ## 0.891506777109113, 0.900326480050901, 0.900957189878304, 0.907610891690096, ## 0.823758306599425, 0.878047227944174, 0.887758060106899, 0.875873457555894, ## 0.588537798996413, 0.880058632190315, 0.969667237978431, 0.61335967578347, ## 0.905397692113209, 0.510448063910782, 0.956721439561639, 0.741900459014366, ## 0.840512847259059, 0.378443055140036, 0.792289990504109, 0.886751188757628, ## 0.97827881282847, 0.653797227471872, 0.940314935818708, 0.791692308717914, ## 0.825300523077781, 0.988029276107241, 0.717304558086278, 0.891338733375895, ## 0.921268238448035, 0.96882033048144, 0.806311803637692, 0.905332905383967, ## 0.235933916206776, 0.497410856153194, 0.978938035328761, 0.69018751378284, ## 0.92953957203868, 0.791257146320022, 0.919785663412833, 0.919633602318112, ## 0.871750135539322, 0.976282431271559, 0.908034365704355, 0.58726308219449, ## 0.781110212673502, 0.573105225656775, 0.920257020668673, 0.318474365973356, ## 0.680838815388414, 0.979495022241474, 0.824274082372844, 0.931825818361673, ## 0.683809067833928, 0.889218471780631, 0.979210080081063, 0.882748830653636, ## 0.148877466501482, 0.452510389167275, 0.706422294812868, 0.73354503694632, ## 0.554843644996668, 0.866377920299765, 0.769203904604713, 0.830372485452739, ## 0.68520963282006, 0.651742821698432, 0.845783610607634, 0.922393502893228, ## 0.945557056601795, 0.583779803535785, 0.0731022923729763, 0.934701764316106, ## 0.0645712751005522, 0.874870790707177, 0.918591534975684, 0.838170153087413, ## 0.987143357409401, 0.270540734045784, 0.850144124353865, 0.949196559441712, ## 0.351849688077706, 0.926442784028319, 0.899226886951712, 0.723911834959543, ## 0.863376217423714, 0.882703291100244, 0.767559937935336, 0.885523144373898, ## 0.679011864642318, 0.880201945429299, 0.839709614405011, 0.935782832232867, ## 0.932470648685389, 0.472323575808305, 0.792137712296657, 0.797418773799896, ## 0.982853942774392, 0.765234129767905, 0.945130874524734, 0.758442104492331, ## 0.957952785245067, 0.973862764112691, 0.906668726444364, 0.875071736191179, ## 0.884081831719927, 0.762533119986748, 0.700979023318505, 0.804226109011326, ## 0.734059594636249, 0.877189135375166, 0.9106167984677, 0.970336157499332, ## 0.725141417704862, 0.918551244137315, 0.943700844649053, 0.724995435279486, ## 0.411417236952838, 0.234179028210345, 0.893086717145203, 0.909791507979313, ## 0.770424947702005, 0.937924835852395, 0.906510924604849, 0.794523419861598, ## 0.862811427154031, 0.713721737443966, 0.351890112773157, 0.813570761438878, ## 0.900832595185882, 0.831442446563805, 0.92578148885952, 0.902008342541673, ## 0.843240749708291, 0.859015726993071, 0.470110893438645, 0.9347117684088, ## 0.914869554485669, 0.828727711293772, 0.884443504273863, 0.697396501577209, ## 0.726374081649796, 0.721770195595487, 0.511465667553365, 0.854134991601238, ## 0.517618270246465, 0.692711406842704, 0.949231450134155, 0.907078597579348, ## 0.965998163267573, 0.831248284775068, 0.828903965533408, 0.592016205764271, ## 0.903303404430717, 0.902451157571832, 0.904910385913459, 0.923253161382803, ## 0.813309607554033, 0.862212139698164, 0.657446569921923, 0.966831799887284, ## 0.552985118564986, 0.946676473111894, 0.865088654073566, 0.770383373534952, ## 0.913828065935482, 0.850150846087876, 0.987489328096584, 0.834097980180465, ## 0.851014686687953, 0.0282688534578425, 0.88133390961873, 0.903840966659902, ## 0.86949536496132, 0.895090137406537, 0.777339895356093, 0.598919777861358, ## 0.937535692676443, 0.906367829683814, 0.898709699071897, 0.89439355333744, ## 0.840871378578854, 0.862884710948128, 0.92411864972548, 0.973934273311574, ## 0.916291628242461, 0.664850111577235, 0.758955813738078, 0.877997872578135, ## 0.475917138811541, 0.926816199102565, 0.954180143482695, 0.77698097197627, ## 0.6626506363957, 0.809728826060718, 0.898812407846538, 0.958607103761765, ## 0.551033213242686, 0.386862702268625, 0.264951258885428, 0.865261320252258, ## 0.760609469771247, 0.34114434356426, 0.204043324021249, 0.917150695000056, ## 0.930096882228182, 0.340350727735597, 0.643914556025731, 0.923895795997331, ## 0.929562783709472, 0.986416584183986, 0.832551405345267, 0.888623095024728, ## 0.725894296453692, 0.830729847992192, 0.888486533489889, 0.782797504512506, ## 0.773143072461289, 0.58366313995514, 0.968389663612765, 0.883806190611145, ## 0.702713569556156, 0.402377274868612, 0.768809828692286, 0.297497658399238, ## 0.903925646353357, 0.862704209463583, 0.760923286209893, 0.546559784793544, ## 0.666853041714529, 0.862472978773876, 0.902008168039061, 0.79872781768231, ## 0.8237610087361, 0.934906585109099, 0.8172369646368, 0.674210085113622, ## 0.855197180564852, 0.686791393544478, 0.809973184378572, 0.728923189797897, ## 0.570036168010861, 0.923273438642983, 0.887118635121896, 0.898749703654332, ## 0.85947307925801, 0.89522078077686, 0.898660879893392, 0.849878884255606, ## 0.828927257697614, 0.912467552211514, 0.0492623975685308, 0.713175109709449, ## 0.717881101415614, 0.756101671000252, 0.886183785764912, 0.546352011569479, ## 0.864645260866284, 0.919410864379319, 0.909503779576341, 0.6489502893021, ## 0.696713731734065, 0.832806938575821, 0.937396081119809, 0.263975321697929, ## 0.205439410940767, 0.139706974090309, 0.94871547811215, 0.284034185850637, ## 0.964420494130639, 0.0915324797860856, 0.42223239222142, 0.37773794695664, ## 0.627993790632512, 0.650841779231974, 0.814873869681773, 0.782276042456964, ## 0.0593910552068254, 0.699580124098259, 0.334316301122192, 0.526204506899719, ## 0.277802644287222, 0.419881749106096, 0.60830099220448, 0.853115140640235, ## 0.467607646262843, 0.070969336640789, 0.0419933035958426, 0.662102946067296, ## 0.150512190685067, 0.110981769044548, 0.49390968549475, 0.661532095440779, ## 0.84900997444138, 0.821838367601395, 0.161787737209113, 0.343646595261188, ## 0.389495709491909, 0.244282591693335, 0.70168774136797, 0.604474063326896, ## 0.157565035735454, 0.937299842800485, 0.404768118376548, 0.309752108138139, ## 0.254975234357488, 0.422688938630679, 0.645820078380003, 0.136484611302083, ## 0.893579709692452, 0.582527528323701, 0.828415557440863, 0.888205903445709, ## 0.329806207582431, 0.314168038025145, 0.305398841490536, 0.620627826102788, ## 0.130726913416767, 0.0407280874281913, 0.11994708712488, 0.01729555935237, ## 0.710594266880224, 0.777274610611156, 0.85685615831858, 0.486176619802039, ## 0.10874627990489, 0.239901504554133, 0.379011815497867, 0.0446366997442029, ## 0.140292007206542, 0.659491582410151, 0.787169509497341, 0.655570453676594, ## 0.290894519298211, 0.0240128775570781, 0.307771465651619, 0.892984917154045, ## 0.687183680181749, 0.629065808022082, 0.0451129506877012, 0.347240710048925, ## 0.116747567319006, 0.47672388184021, 0.715567056108152, 0.0891858049733552, ## 0.724456106328891, 0.872563115433066, 0.648401278903481, 0.342416141509801, ## 0.336121530830524, 0.223075205368535, 0.339062991771028, 0.369498451501146, ## 0.138698828406496, 0.106510364526974, 0.14994636995584, 0.178497329845862, ## 0.71588819249746, 0.546650106370509, 0.0518945128517676, 0.711697639682455, ## 0.803263691414065, 0.0845520198288157, 0.290201814724092, 0.710581123708473, ## 0.305676775026597, 0.480410759672323, 0.277672517315764, 0.830188252309146, ## 0.520957441915155, 0.366262782879567, 0.90125694128485, 0.599026072011316, ## 0.794179351894165, 0.406018348959599, 0.51042095459052, 0.718910869893026, ## 0.560179526713892, 0.142614562628049, 0.599457221810202, 0.504545609040934, ## 0.287515504184717, 0.633086849494241, 0.690856927889885, 0.637536519159075, ## 0.497093462568646, 0.269955811589028, 0.155796058908518, 0.854297510067159, ## 0.854080902793633, 0.225909762212078, 0.695108885924986, 0.82846319377955, ## 0.0653371019184436, 0.717863786781081, 0.243860107658981, 0.0974844516260257, ## 0.99080360265041, 0.2406772828501, 0.633010894234573, 0.0908434301146492, ## 0.127393244829022, 0.134990451230954, 0.250685211630676, 0.742038844760488, ## 0.778482051683754, 0.088758631393759, 0.188516417650311, 0.162051374264702, ## 0.655654959518221, 0.610534955256999, 0.464971895366955, 0.15811570380799, ## 0.338098845168008, 0.784170231804055, 0.173819396905118, 0.883712620228914, ## 0.598853437320952, 0.246045572859086, 0.767559649488715, 0.06113363922658, ## 0.238178918239677, 0.651600556297326, 0.508865297532097, 0.376632870858269, ## 0.794843831384079, 0.216744477528262, 0.200820922417216, 0.243755866071719, ## 0.915253955478841, 0.249534991593007, 0.257679230130395, 0.638117916404094, ## 0.784226215493112, 0.00656595050781807, 0.887941096967722, 0.732089891769394, ## 0.0700239901631858, 0.142902081600351, 0.461473779570138, 0.845792207201813, ## 0.383313256449866, 0.13629906739962, 0.616135945391271, 0.551179825918982, ## 0.144263139361828, 0.990553567019526, 0.348373676499465, 0.778653308581399, ## 0.462708808713126, 0.306965441241999, 0.342783549183306, 0.406032661961144, ## 0.589287956071996, 0.610557408573761, 0.71471759537606, 0.637204304761958, ## 0.147868477939634, 0.0374668583777157, 0.574971104627774, 0.110167175686962, ## 0.830496887457063, 0.619023854856898, 0.0500133988140515, 0.52244345084645, ## 0.723020373204231, 0.362460782238694, 0.144283231256657, 0.239046050826892, ## 0.579604252268163, 0.304761172241834, 0.328139732956354, 0.23890523971643, ## 0.134281661628837, 0.168048215571617, 0.188867475946522, 0.170750416459014, ## 0.232842029801012, 0.536097592060267, 0.69790696562415, 0.673350600523292, ## 0.596851276576304, 0.610367195471754, 0.486973062698443, 0.599027660700978, ## 0.243155262075541, 0.518129027081139, 0.50643447736246, 0.896700682371225, ## 0.0346795522510761, 0.283346564223603, 0.248376296871524, 0.609125139204232, ## 0.417241184958375, 0.744822933293396, 0.106762125626538, 0.388898613280549, ## 0.713134786106623, 0.210332066138096, 0.708152080889416, 0.352023343392863, ## 0.132373213361272, 0.368312080145862, 0.64794660318098, 0.454489297363444, ## 0.634553842047288, 0.262124761024674, 0.898838565615997, 0.203946752406817, ## 0.863257535871388, 0.197966546965742, 0.0784562751670896, 0.222875728007091, ## 0.751846859279304, 0.620417399206161, 0.776159782139887, 0.807999581494565, ## 0.154689277948017, 0.820571620189719, 0.280505538099138, 0.354004894639258, ## 0.351982056368604, 0.158570495340489, 0.411801979158477, 0.460208160968111, ## 0.376280443493286, 0.293145600066582, 0.586622200281909, 0.752587176301719, ## 0.0767645840341628, 0.0621450986478847, 0.75883430175351), knn = c(0.888888888888889, ## 1, 0.555555555555556, 0.444444444444444, 0.666666666666667, 0.333333333333333, ## 0.666666666666667, 0.555555555555556, 0.666666666666667, 0.888888888888889, ## 0.444444444444444, 0.777777777777778, 0.444444444444444, 1, 0.888888888888889, ## 0.222222222222222, 0.666666666666667, 0.555555555555556, 0.666666666666667, ## 0.555555555555556, 0.555555555555556, 0.333333333333333, 0.444444444444444, ## 0.888888888888889, 0.888888888888889, 1, 0.888888888888889, 1, ## 0.222222222222222, 1, 0.666666666666667, 0.444444444444444, 0.777777777777778, ## 1, 1, 0.555555555555556, 1, 0.666666666666667, 0.888888888888889, ## 0.333333333333333, 0.555555555555556, 0.777777777777778, 0.888888888888889, ## 0.777777777777778, 1, 0.777777777777778, 0.888888888888889, 0.777777777777778, ## 1, 0.888888888888889, 1, 0.777777777777778, 0.555555555555556, ## 0.777777777777778, 1, 1, 0.555555555555556, 1, 0.777777777777778, ## 0.555555555555556, 1, 1, 0.888888888888889, 0.555555555555556, ## 0.777777777777778, 1, 0.888888888888889, 0.777777777777778, 0.888888888888889, ## 0.555555555555556, 1, 0.888888888888889, 1, 1, 0.888888888888889, ## 0.888888888888889, 0.666666666666667, 0.666666666666667, 0.555555555555556, ## 0.777777777777778, 0.777777777777778, 0.777777777777778, 0.888888888888889, ## 0.666666666666667, 1, 0.888888888888889, 0.555555555555556, 0.777777777777778, ## 0.666666666666667, 0.333333333333333, 0.777777777777778, 0.666666666666667, ## 1, 0.888888888888889, 0.777777777777778, 0.444444444444444, 0.888888888888889, ## 0.555555555555556, 0.555555555555556, 0.333333333333333, 1, 1, ## 1, 0.444444444444444, 0.666666666666667, 1, 1, 0.555555555555556, ## 0.555555555555556, 0.666666666666667, 0.888888888888889, 1, 1, ## 1, 0.888888888888889, 0.333333333333333, 0.888888888888889, 0.777777777777778, ## 0.888888888888889, 1, 0.555555555555556, 1, 0.555555555555556, ## 0.777777777777778, 1, 0.777777777777778, 0.444444444444444, 0.888888888888889, ## 1, 0.555555555555556, 0.777777777777778, 0.888888888888889, 0.888888888888889, ## 0.444444444444444, 0.111111111111111, 0.666666666666667, 0.666666666666667, ## 0.222222222222222, 0.888888888888889, 0.888888888888889, 0.666666666666667, ## 0.222222222222222, 0.777777777777778, 0.888888888888889, 0.666666666666667, ## 1, 0.888888888888889, 1, 0.888888888888889, 0.444444444444444, ## 0.222222222222222, 0.222222222222222, 0.666666666666667, 0.777777777777778, ## 0.222222222222222, 0.555555555555556, 1, 1, 0.555555555555556, ## 0.888888888888889, 0.444444444444444, 0.111111111111111, 1, 0.555555555555556, ## 1, 0.666666666666667, 1, 0.666666666666667, 0.555555555555556, ## 1, 0.555555555555556, 0.888888888888889, 1, 0.444444444444444, ## 0.888888888888889, 0.444444444444444, 0.444444444444444, 0.333333333333333, ## 0.111111111111111, 0.888888888888889, 0.777777777777778, 0.888888888888889, ## 0.666666666666667, 0.444444444444444, 0.777777777777778, 0.666666666666667, ## 1, 0.555555555555556, 0.555555555555556, 0.777777777777778, 0.888888888888889, ## 0.777777777777778, 0.888888888888889, 0.777777777777778, 1, 0.777777777777778, ## 0.777777777777778, 0.777777777777778, 0.333333333333333, 0, 0.777777777777778, ## 0.333333333333333, 1, 0.444444444444444, 0.888888888888889, 0.333333333333333, ## 0.444444444444444, 0.888888888888889, 1, 0.777777777777778, 0.444444444444444, ## 0.888888888888889, 0.444444444444444, 0.777777777777778, 0.666666666666667, ## 1, 0.777777777777778, 0.666666666666667, 0.888888888888889, 1, ## 0.888888888888889, 1, 0.333333333333333, 0.333333333333333, 0.666666666666667, ## 0.777777777777778, 1, 0.888888888888889, 1, 1, 0.888888888888889, ## 0.444444444444444, 0.888888888888889, 1, 0.777777777777778, 0.888888888888889, ## 1, 0.777777777777778, 1, 0.888888888888889, 1, 0.777777777777778, ## 0.444444444444444, 0.333333333333333, 0.666666666666667, 0.777777777777778, ## 1, 1, 0.555555555555556, 0.777777777777778, 0.333333333333333, ## 1, 0.666666666666667, 0.444444444444444, 0.666666666666667, 0.888888888888889, ## 1, 1, 0.333333333333333, 1, 0.777777777777778, 0.888888888888889, ## 0.888888888888889, 0.777777777777778, 0.777777777777778, 0.888888888888889, ## 1, 0.888888888888889, 0.888888888888889, 0.555555555555556, 0.555555555555556, ## 1, 0.777777777777778, 1, 0.888888888888889, 1, 0.777777777777778, ## 0.777777777777778, 0.888888888888889, 0.555555555555556, 0.444444444444444, ## 1, 0.444444444444444, 0.888888888888889, 0.555555555555556, 0.777777777777778, ## 1, 1, 0.777777777777778, 0.888888888888889, 0.888888888888889, ## 1, 0.888888888888889, 0.555555555555556, 0.666666666666667, 0.666666666666667, ## 0.777777777777778, 0.444444444444444, 0.888888888888889, 0.555555555555556, ## 0.888888888888889, 0.666666666666667, 0.555555555555556, 0.666666666666667, ## 1, 0.888888888888889, 0.222222222222222, 0.222222222222222, 1, ## 0.333333333333333, 0.888888888888889, 0.555555555555556, 0.777777777777778, ## 1, 0.333333333333333, 0.666666666666667, 1, 0.222222222222222, ## 0.888888888888889, 0.888888888888889, 0.888888888888889, 0.666666666666667, ## 1, 0.444444444444444, 0.888888888888889, 0.777777777777778, 0.777777777777778, ## 1, 0.888888888888889, 0.888888888888889, 0.444444444444444, 1, ## 0.555555555555556, 0.888888888888889, 0.888888888888889, 0.777777777777778, ## 0.777777777777778, 1, 0.888888888888889, 1, 0.666666666666667, ## 1, 0.888888888888889, 0.777777777777778, 0.888888888888889, 0.888888888888889, ## 1, 0.666666666666667, 1, 0.777777777777778, 0.777777777777778, ## 0.888888888888889, 0.444444444444444, 0.555555555555556, 0.222222222222222, ## 0.777777777777778, 1, 0.777777777777778, 1, 1, 0.777777777777778, ## 0.777777777777778, 0.666666666666667, 0.555555555555556, 0.888888888888889, ## 0.888888888888889, 0.888888888888889, 0.888888888888889, 0.888888888888889, ## 1, 0.777777777777778, 0.222222222222222, 1, 0.777777777777778, ## 0.777777777777778, 1, 0.555555555555556, 0.777777777777778, 1, ## 0.333333333333333, 0.888888888888889, 0.666666666666667, 0.444444444444444, ## 1, 0.666666666666667, 1, 0.888888888888889, 1, 0.444444444444444, ## 1, 1, 1, 0.777777777777778, 0.888888888888889, 0.555555555555556, ## 0.777777777777778, 1, 0.444444444444444, 0.888888888888889, 0.666666666666667, ## 0.888888888888889, 0.777777777777778, 0.888888888888889, 1, 0.666666666666667, ## 0.888888888888889, 0, 0.888888888888889, 0.888888888888889, 0.888888888888889, ## 0.666666666666667, 0.666666666666667, 0.222222222222222, 1, 0.777777777777778, ## 1, 0.777777777777778, 0.888888888888889, 0.444444444444444, 1, ## 0.888888888888889, 0.888888888888889, 0.555555555555556, 0.888888888888889, ## 0.666666666666667, 0.333333333333333, 0.888888888888889, 1, 0.888888888888889, ## 0.666666666666667, 0.444444444444444, 0.888888888888889, 1, 0.333333333333333, ## 0.777777777777778, 0.222222222222222, 0.777777777777778, 0.888888888888889, ## 0.333333333333333, 0.222222222222222, 1, 1, 0.666666666666667, ## 0.777777777777778, 0.888888888888889, 0.888888888888889, 1, 0.777777777777778, ## 0.666666666666667, 0.777777777777778, 0.555555555555556, 0.777777777777778, ## 0.555555555555556, 0.666666666666667, 0.777777777777778, 0.888888888888889, ## 0.777777777777778, 0.444444444444444, 0.666666666666667, 0.666666666666667, ## 0.222222222222222, 0.777777777777778, 0.777777777777778, 0.555555555555556, ## 0.444444444444444, 0.444444444444444, 0.888888888888889, 0.666666666666667, ## 0.555555555555556, 0.777777777777778, 0.888888888888889, 0.666666666666667, ## 0.666666666666667, 0.777777777777778, 0.222222222222222, 0.666666666666667, ## 0.888888888888889, 0.111111111111111, 0.888888888888889, 0.555555555555556, ## 0.555555555555556, 0.888888888888889, 0.777777777777778, 0.777777777777778, ## 0.777777777777778, 0.888888888888889, 1, 0.222222222222222, 0.666666666666667, ## 0.888888888888889, 0.777777777777778, 0.888888888888889, 0.555555555555556, ## 0.555555555555556, 1, 0.888888888888889, 0.444444444444444, 0.666666666666667, ## 0.777777777777778, 1, 0.555555555555556, 0.222222222222222, 0.444444444444444, ## 1, 0.222222222222222, 1, 0.111111111111111, 0.333333333333333, ## 0.222222222222222, 0.888888888888889, 0.555555555555556, 0.666666666666667, ## 0.888888888888889, 0.222222222222222, 0.888888888888889, 0.222222222222222, ## 0.666666666666667, 0.444444444444444, 0.333333333333333, 0.555555555555556, ## 0.888888888888889, 0.666666666666667, 0.333333333333333, 0, 0.888888888888889, ## 0.222222222222222, 0.222222222222222, 0.555555555555556, 0.333333333333333, ## 0.555555555555556, 1, 0.666666666666667, 0.222222222222222, 0.222222222222222, ## 0.555555555555556, 0.777777777777778, 0.666666666666667, 0, 1, ## 0.444444444444444, 0.333333333333333, 0.222222222222222, 0.555555555555556, ## 0.333333333333333, 0.333333333333333, 0.666666666666667, 1, 0.666666666666667, ## 0.888888888888889, 0.222222222222222, 0.555555555555556, 0.111111111111111, ## 0.444444444444444, 0.222222222222222, 0.222222222222222, 0.444444444444444, ## 0.222222222222222, 0.777777777777778, 0.444444444444444, 0.666666666666667, ## 0.666666666666667, 0.333333333333333, 0.555555555555556, 0.555555555555556, ## 0.111111111111111, 0.333333333333333, 0.777777777777778, 0.888888888888889, ## 0.555555555555556, 0.333333333333333, 0.333333333333333, 0.444444444444444, ## 1, 0.888888888888889, 0.777777777777778, 0.111111111111111, 0.222222222222222, ## 0.111111111111111, 0.777777777777778, 0.555555555555556, 0.555555555555556, ## 1, 1, 0.222222222222222, 0.333333333333333, 0.444444444444444, ## 0.333333333333333, 0.666666666666667, 0.666666666666667, 0.111111111111111, ## 0.111111111111111, 0.111111111111111, 0, 0.777777777777778, 0.666666666666667, ## 0.222222222222222, 0.888888888888889, 0.888888888888889, 0.333333333333333, ## 0.222222222222222, 0.444444444444444, 0.222222222222222, 0.111111111111111, ## 0.555555555555556, 1, 0.444444444444444, 0.222222222222222, 0.666666666666667, ## 0.666666666666667, 0.888888888888889, 0.666666666666667, 0.555555555555556, ## 0.444444444444444, 0.777777777777778, 0, 0.666666666666667, 0.666666666666667, ## 0.444444444444444, 0.888888888888889, 0.888888888888889, 0.555555555555556, ## 0.333333333333333, 0.222222222222222, 0.333333333333333, 0.666666666666667, ## 0.666666666666667, 0.444444444444444, 0.777777777777778, 0.888888888888889, ## 0.111111111111111, 0.333333333333333, 0.555555555555556, 0.111111111111111, ## 0.888888888888889, 0.111111111111111, 0.555555555555556, 0.444444444444444, ## 0.111111111111111, 0.222222222222222, 0.555555555555556, 0.666666666666667, ## 0.444444444444444, 0.444444444444444, 0.444444444444444, 0.222222222222222, ## 0.555555555555556, 0.555555555555556, 0.666666666666667, 0.111111111111111, ## 0.222222222222222, 0.777777777777778, 0, 1, 0.777777777777778, ## 0.222222222222222, 0.444444444444444, 0.111111111111111, 0.222222222222222, ## 0.555555555555556, 0.444444444444444, 0.555555555555556, 0.777777777777778, ## 0.666666666666667, 0.333333333333333, 0.111111111111111, 0.666666666666667, ## 0.111111111111111, 0.222222222222222, 0.555555555555556, 0.777777777777778, ## 0, 0.888888888888889, 0.666666666666667, 0, 0.444444444444444, ## 0.444444444444444, 0.888888888888889, 0.444444444444444, 0.111111111111111, ## 0.666666666666667, 0.666666666666667, 0.222222222222222, 0.888888888888889, ## 0.333333333333333, 0.555555555555556, 0.555555555555556, 0.222222222222222, ## 0.333333333333333, 0.222222222222222, 0.555555555555556, 0.555555555555556, ## 0.666666666666667, 0.555555555555556, 0.111111111111111, 0.111111111111111, ## 0.444444444444444, 0.444444444444444, 0.777777777777778, 0.777777777777778, ## 0.111111111111111, 0.666666666666667, 0.222222222222222, 0.555555555555556, ## 0.222222222222222, 0.555555555555556, 0.444444444444444, 0.222222222222222, ## 0.111111111111111, 0.333333333333333, 0, 0.333333333333333, 0.222222222222222, ## 0.333333333333333, 0.333333333333333, 0.555555555555556, 0.666666666666667, ## 0.666666666666667, 0.555555555555556, 0.888888888888889, 0.666666666666667, ## 0.222222222222222, 0.111111111111111, 0.444444444444444, 0.333333333333333, ## 1, 0, 0.333333333333333, 0.333333333333333, 0.555555555555556, ## 0.777777777777778, 0.555555555555556, 0.111111111111111, 0.333333333333333, ## 0.666666666666667, 0.333333333333333, 0.666666666666667, 0.444444444444444, ## 0.222222222222222, 0.444444444444444, 0.333333333333333, 0.222222222222222, ## 0.555555555555556, 0.222222222222222, 0.888888888888889, 0.111111111111111, ## 0.888888888888889, 0.666666666666667, 0.555555555555556, 0.333333333333333, ## 0.444444444444444, 0.555555555555556, 0.888888888888889, 0.777777777777778, ## 0.333333333333333, 0.333333333333333, 0.333333333333333, 0.555555555555556, ## 0.555555555555556, 0.111111111111111, 0.555555555555556, 0.777777777777778, ## 0.222222222222222, 0.444444444444444, 0.666666666666667, 0.333333333333333, ## 0.222222222222222, 0, 0.333333333333333), .outcome = c(1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)), control = list(minsplit = 20, ## minbucket = 7, cp = 0, maxcompete = 4, maxsurrogate = 5, ## usesurrogate = 2, surrogatestyle = 0, maxdepth = 30, xval = 0)) ## n= 768 ## ## CP nsplit rel error ## 1 0.35074627 0 1.0000000 ## 2 0.02238806 1 0.6492537 ## ## Variable importance ## lda glm knn ## 39 38 24 ## ## Node number 1: 768 observations, complexity param=0.3507463 ## predicted class=neg expected loss=0.3489583 P(node) =1 ## class counts: 500 268 ## probabilities: 0.651 0.349 ## left son=2 (460 obs) right son=3 (308 obs) ## Primary splits: ## lda &lt; 0.6623768 to the right, improve=94.81988, (0 missing) ## glm &lt; 0.6494944 to the right, improve=92.00008, (0 missing) ## knn &lt; 0.7222222 to the right, improve=59.80801, (0 missing) ## Surrogate splits: ## glm &lt; 0.6515483 to the right, agree=0.990, adj=0.974, (0 split) ## knn &lt; 0.6111111 to the right, agree=0.846, adj=0.617, (0 split) ## ## Node number 2: 460 observations ## predicted class=neg expected loss=0.1456522 P(node) =0.5989583 ## class counts: 393 67 ## probabilities: 0.854 0.146 ## ## Node number 3: 308 observations ## predicted class=pos expected loss=0.3474026 P(node) =0.4010417 ## class counts: 107 201 ## probabilities: 0.347 0.653 # Blending (linear combination of models) # load libraries library(caret) library(caretEnsemble) # load the dataset data(PimaIndiansDiabetes) # define training control train_control &lt;- trainControl(method=&quot;cv&quot;, number=10, returnData = TRUE, returnResamp = &quot;final&quot;, savePredictions = FALSE, classProbs=TRUE) # train a list of models methodList &lt;- c(&#39;glm&#39;, &#39;lda&#39;, &#39;knn&#39;) models &lt;- caretList(diabetes~., data=PimaIndiansDiabetes, trControl=train_control, methodList=methodList) # create ensemble of trained models ensemble &lt;- caretEnsemble(models) # summarize ensemble summary(ensemble) ## The following models were ensembled: glm, lda, knn ## They were weighted: ## 2.4958 -9.1523 4.4327 -0.3551 ## The resulting Accuracy is: 0.7672 ## The fit for each individual model on the Accuracy is: ## method Accuracy AccuracySD ## glm 0.7746070 0.04546725 ## lda 0.7693951 0.04383666 ## knn 0.7343643 0.06912261 # Bagging or Bootstrap Aggregation of Decision Trees # load the libraries library(ipred) library(rpart) library(mlbench) # load the dataset data(PimaIndiansDiabetes) # bag the decision tree model &lt;- bagging(diabetes~., data=PimaIndiansDiabetes, nbagg=25, coob=TRUE) # make predictions on the training dataset predictions &lt;- predict(model, PimaIndiansDiabetes[,1:8]) # summarize accuracy table(predictions, PimaIndiansDiabetes$diabetes) ## ## predictions neg pos ## neg 498 1 ## pos 2 267 "]
]
